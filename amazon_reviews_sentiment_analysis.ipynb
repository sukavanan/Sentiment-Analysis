{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6af33410",
   "metadata": {},
   "source": [
    "# CSCI_544 - HW2 - Sukavanan Nanjundan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301c992",
   "metadata": {},
   "source": [
    "Note to the grader: If you want to run the whole code, I suggesst skipping the model training process and use the pretrained model files that I've included. If you run into memory errors anywhere, please just restart the kernel and run from the previous checkpoints (which I have mentioned). You could also manually increase the virtual memory of the system just for the duration of running this file to ensure a smoother process. I might delete some variables at some point to reduce memory usage and speed up the training process, please disregard those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a65d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import copy\n",
    "import warnings\n",
    "import gc\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "from sklearn.utils import class_weight\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857990e7",
   "metadata": {},
   "source": [
    "First, we are reading the TSV file and storing it as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3829283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/amazon_reviews_us_Kitchen_v1_00.tsv\", sep = '\\t', error_bad_lines = False, warn_bad_lines = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a262304b",
   "metadata": {},
   "source": [
    "Now, dropping the rows with NAN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f43871",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed03bfee",
   "metadata": {},
   "source": [
    "Dropping all the columns except the review_body and the star_rating columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d386a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(data.columns)\n",
    "columns.remove('review_body')\n",
    "columns.remove('star_rating')\n",
    "df = data.drop(columns, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6cc628",
   "metadata": {},
   "source": [
    "Cleaning the data by converting it to lowercase, removing urls and html data and also by removing extra white spaces between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68b9f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review_body = df.review_body.str.lower()\n",
    "df.review_body = df.review_body.replace(\"http\\S+\", \"\", regex = True).replace(\"www\\S+\", \"\", regex = True).replace(\"<.*?>\", \"\", regex = True)\n",
    "df.review_body = df.review_body.replace(\"[^a-zA-Z ]\", \" \", regex = True)\n",
    "df.review_body = df.review_body.replace(\" +\", \" \", regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1d6df7",
   "metadata": {},
   "source": [
    "Once again, dropping any null or NAN values that might have arised after the data cleaning process and storing the data for easier access in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e1f0c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.isnull().any()\n",
    "df.to_csv(\"data/amazon_reviews_cleaned.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479a4739",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/amazon_reviews_cleaned.csv\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01630f49",
   "metadata": {},
   "source": [
    "Now, we select 50,0000 samples for each rating and then use that as our new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c09fea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_1 = df[df.star_rating == 1].sample(n = 50000, random_state = 42)\n",
    "rat_2 = df[df.star_rating == 2].sample(n = 50000, random_state = 42)\n",
    "rat_3 = df[df.star_rating == 3].sample(n = 50000, random_state = 42)\n",
    "rat_4 = df[df.star_rating == 4].sample(n = 50000, random_state = 42)\n",
    "rat_5 = df[df.star_rating == 5].sample(n = 50000, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d92c0a9",
   "metadata": {},
   "source": [
    "Concatenating the samples to form one dataframe and replacing the star ratings with the values 0, 1, 2 representing negative (0, 1 ratings), positive (4, 5 ratings) and neutral (3 rating) sentiments of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "acf97471",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [rat_1, rat_2, rat_3, rat_4, rat_5]\n",
    "df_new = pd.concat(frames, ignore_index = True)\n",
    "df_new = df_new.replace({'star_rating': {1: 0, 2: 0, 4: 1, 5: 1, 3: 2}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb1db800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c7e64e",
   "metadata": {},
   "source": [
    "# Creating the word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c45d11",
   "metadata": {},
   "source": [
    "##### Checkpoint - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63083057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v_google = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd66a8a",
   "metadata": {},
   "source": [
    "As you can see, the google word2vec model has been trained on a large and meaningful data such that the vector represantations of the words are semantically correct, as corroborated by the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a49f6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_google.most_similar(positive = ['king', 'woman'], negative = ['man'], topn = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7638c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55674857"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_google.similarity(\"excellent\", \"outstanding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0874354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "import gensim.models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc31f69",
   "metadata": {},
   "source": [
    "Now, we define a corpus class that iterates over each review in our dataframe and gives that as input to the word2vec model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "657f89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        for i in range(len(df_new)):\n",
    "            yield utils.simple_preprocess(str(df_new.iloc[[i]].review_body.values[0]))\n",
    "\n",
    "sentences = MyCorpus()\n",
    "w2v_model = gensim.models.Word2Vec(sentences = sentences, vector_size = 300, min_count = 10, window = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cdccf0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(\"model/gensim_w2v_amazon_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f8bb97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec.load(\"model/gensim_w2v_amazon_reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33486409",
   "metadata": {},
   "source": [
    "As you can see, the word2vec model trained just using the 250K reviews has not been able to produce the same level of semantic correctness when compared to the google word2vec model as proven by the 'king' + 'woman' - 'man' example. But, in the subsequent examples of comparing the similarity between \"outstanding\" and \"excellent\", and 'utensil' + 'sharp', the model seems to perform well. This might be due to the fact that the context of the latter examples would be closely related to that of  a review and the model would have been able to capture their semantic similarities with more accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f73fcdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shams', 0.44163042306900024),\n",
       " ('qt', 0.41580283641815186),\n",
       " ('cambro', 0.41324180364608765),\n",
       " ('quart', 0.4047726094722748),\n",
       " ('americolor', 0.3984954357147217),\n",
       " ('isi', 0.39528515934944153),\n",
       " ('arthur', 0.39269858598709106),\n",
       " ('elite', 0.3870254158973694),\n",
       " ('cd', 0.38650548458099365),\n",
       " ('jumbo', 0.3864506185054779)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive = ['king', 'woman'], negative = ['man'], topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "79a94d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scissors', 0.5846854448318481),\n",
       " ('knife', 0.581675112247467),\n",
       " ('knifes', 0.5791575312614441),\n",
       " ('utensils', 0.5731496214866638),\n",
       " ('serrated', 0.5610765814781189)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive = [\"utensil\", \"sharp\"], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b7be293f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77758265"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity(\"excellent\", \"outstanding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0619ad",
   "metadata": {},
   "source": [
    "Performing data pre-processing tasks such as contractions, stop-word removal and lemmatization. Pre-processing helps in improving the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46a3a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Performing Contractions on the reviews\n",
    "df_new.review_body = df_new.review_body.replace(df_new.review_body.values[0], contractions.fix(df_new.review_body.values[0]).lower())\n",
    "\n",
    "#Performing stop word removal on the reviews\n",
    "stop = stopwords.words(\"english\")\n",
    "df_new.review_body = df_new.review_body.apply(lambda x: \" \".join([word for word in x.split() if word not in stop]))\n",
    "\n",
    "#Performing Lemmatization on the reviews\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df_new.review_body = df_new.review_body.apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split(\" \")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589037f",
   "metadata": {},
   "source": [
    "Saving the dataframe for easier access in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76fbf881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.dropna()\n",
    "df_new.to_csv(\"data/50k_samples.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb2b41b",
   "metadata": {},
   "source": [
    "##### Checkpoint - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c55f84ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv(\"data/50k_samples.csv\")\n",
    "df_new = df_new.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bced55d",
   "metadata": {},
   "source": [
    "Below, is the function that returns the average of the vectors of each word that is produced by the given word2vec model (google or amazon reviews). We ignore the words that are not present in the given model using the try, except block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "15f5451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_w2v(sentence, model):\n",
    "    result_vector = np.ndarray(shape = (300,), buffer = np.zeros((300,)), dtype = float)\n",
    "    removed = 0\n",
    "    if model == 'google':\n",
    "        for word in sentence.split(\" \"):\n",
    "            try:\n",
    "                result_vector = result_vector + w2v_google[word]\n",
    "            except KeyError:\n",
    "                removed += 1\n",
    "                continue\n",
    "    else:\n",
    "        for word in sentence.split(\" \"):\n",
    "            try:\n",
    "                result_vector = result_vector + w2v_model.wv[word]\n",
    "            except KeyError:\n",
    "                removed += 1\n",
    "                continue\n",
    "    return list(result_vector / (len(sentence) - removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27ab23",
   "metadata": {},
   "source": [
    "Now, we create a list of tuples where the first element is the average vector of the given review as calculated by the above mentioned function and the second element is the label for that review. We do this for both the google and the amazon word2vec models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a81f8140",
   "metadata": {},
   "outputs": [],
   "source": [
    "google = [(compute_average_w2v(df_new.iloc[[x]].review_body.values[0], \"google\"), df_new.iloc[[x]].star_rating.values[0]) for x in range(len(df_new))]\n",
    "amazon = [(compute_average_w2v(df_new.iloc[[x]].review_body.values[0], \"amazon\"), df_new.iloc[[x]].star_rating.values[0]) for x in range(len(df_new))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732bf35a",
   "metadata": {},
   "source": [
    "Then, we remove any null or NAN values from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fef45bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "google = [google[x] for x in range(len(google)) if np.isnan(google[x][0]).any() == False]\n",
    "amazon = [amazon[x] for x in range(len(amazon)) if np.isnan(amazon[x][0]).any() == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c53fb7",
   "metadata": {},
   "source": [
    "Now, we convert the list of tuples into a dataframe and rename the columns as vector and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "04e957f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_df = pd.DataFrame(google)\n",
    "amazon_df = pd.DataFrame(amazon)\n",
    "google_df = google_df.rename(columns = {0: \"vector\", 1: \"label\"})\n",
    "amazon_df = amazon_df.rename(columns = {0: \"vector\", 1: \"label\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ddbf6f",
   "metadata": {},
   "source": [
    "Then, we create 300 columns to store the values of each individual dimension of the word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8e7115c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_df[['vector' + '_{}'.format(x) for x in range(300)]] = pd.DataFrame(google_df.vector.to_list(), index = google_df.index)\n",
    "amazon_df[['vector' + '_{}'.format(x) for x in range(300)]] = pd.DataFrame(amazon_df.vector.to_list(), index = amazon_df.index)\n",
    "google_df = google_df.drop(['vector'], axis = 1)\n",
    "amazon_df = amazon_df.drop(['vector'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642693a5",
   "metadata": {},
   "source": [
    "Storing the dataframes as CSV files for easier access in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fc44ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_df.to_csv(\"data/google_model.csv\")\n",
    "amazon_df.to_csv(\"data/amazon_model.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292cbbc0",
   "metadata": {},
   "source": [
    "##### Checkpoint - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e195888",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_df = pd.read_csv(\"data/google_model.csv\")\n",
    "amazon_df = pd.read_csv(\"data/amazon_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8088dd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_binary = google_df[google_df[\"label\"] != 2]\n",
    "amazon_binary = amazon_df[amazon_df[\"label\"] != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "639de80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_google = google_binary.drop(['label'], axis = 1).values\n",
    "y_google = google_binary.label.values\n",
    "x_amazon = amazon_binary.drop(['label'], axis = 1).values\n",
    "y_amazon = amazon_binary.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cf074e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_df = df_new[df_new[\"star_rating\"] != 2]\n",
    "x_tfidf = bin_df.review_body\n",
    "y_tfidf = bin_df.star_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aad987",
   "metadata": {},
   "source": [
    "Splitting the data into training and testing sets. 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e30c7317",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_google_train, x_google_test, y_google_train, y_google_test = train_test_split(x_google, y_google, test_size=0.2, random_state=42)\n",
    "x_amazon_train, x_amazon_test, y_amazon_train, y_amazon_test = train_test_split(x_amazon, y_amazon, test_size=0.2, random_state=42)\n",
    "x_tfidf_train, x_tfidf_test, y_tfidf_train, y_tfidf_test = train_test_split(x_tfidf, y_tfidf, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542564d5",
   "metadata": {},
   "source": [
    "Initializing a TF_IDF model and training it with our binary class dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "410ec060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf_extractor = TfidfVectorizer()\n",
    "x_tfidf_train = tf_idf_extractor.fit_transform(x_tfidf_train)\n",
    "x_tfidf_test = tf_idf_extractor.transform(x_tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a455de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "perceptron = Perceptron()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd4cb5",
   "metadata": {},
   "source": [
    "## Simple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9742b595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of perceptron model trained using Google Word2Vec features: \n",
      "Testing Accuracy:  0.613075\n",
      "Testing Precision:  0.9622448979591837\n",
      "Testing Recall:  0.23566751636927075\n",
      "Testing F1 Score:  0.3786084233348055\n"
     ]
    }
   ],
   "source": [
    "#Perceptron trained using Google W2V features\n",
    "perceptron.fit(x_google_train, y_google_train)\n",
    "y_pred_test = perceptron.predict(x_google_test)\n",
    "print(\"Statistics of perceptron model trained using Google Word2Vec features: \")\n",
    "print(\"Testing Accuracy: \", accuracy_score(y_google_test, y_pred_test))\n",
    "print(\"Testing Precision: \", precision_score(y_google_test, y_pred_test))\n",
    "print(\"Testing Recall: \", recall_score(y_google_test, y_pred_test))\n",
    "print(\"Testing F1 Score: \", f1_score(y_google_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7404b0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of perceptron model trained using Amazon Word2Vec features: \n",
      "Testing Accuracy:  0.7914145707285364\n",
      "Testing Precision:  0.7893648449039882\n",
      "Testing Recall:  0.7977310046275563\n",
      "Testing F1 Score:  0.7935258742297127\n"
     ]
    }
   ],
   "source": [
    "#Perceptron trained using Amazon W2V features\n",
    "perceptron.fit(x_amazon_train, y_amazon_train)\n",
    "y_pred_test = perceptron.predict(x_amazon_test)\n",
    "print(\"Statistics of perceptron model trained using Amazon Word2Vec features: \")\n",
    "print(\"Testing Accuracy: \", accuracy_score(y_amazon_test, y_pred_test))\n",
    "print(\"Testing Precision: \", precision_score(y_amazon_test, y_pred_test))\n",
    "print(\"Testing Recall: \", recall_score(y_amazon_test, y_pred_test))\n",
    "print(\"Testing F1 Score: \", f1_score(y_amazon_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ce7ad2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of perceptron model trained using TF_IDF features: \n",
      "Testing Accuracy:  0.82335\n",
      "Testing Precision:  0.8307857471499412\n",
      "Testing Recall:  0.8122657070025491\n",
      "Testing F1 Score:  0.8214213505863324\n"
     ]
    }
   ],
   "source": [
    "#Perceptron trained using TF_IDF W2V features\n",
    "perceptron.fit(x_tfidf_train, y_tfidf_train)\n",
    "y_pred_test = perceptron.predict(x_tfidf_test)\n",
    "print(\"Statistics of perceptron model trained using TF_IDF features: \")\n",
    "print(\"Testing Accuracy: \", accuracy_score(y_tfidf_test, y_pred_test))\n",
    "print(\"Testing Precision: \", precision_score(y_tfidf_test, y_pred_test))\n",
    "print(\"Testing Recall: \", recall_score(y_tfidf_test, y_pred_test))\n",
    "print(\"Testing F1 Score: \", f1_score(y_tfidf_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f184b00",
   "metadata": {},
   "source": [
    "As we can see the the perceptron model performs the best when the TF_IDF features are used. This might be due to the fact that the perceptron is inherently a very simple model and wasn't able to capture the complex dependencies between the word2vec features and the labels in our dataset. We also notice that the amazon reviews model outperfoms the google word2vec model. This is due to the fact that the we trained our word2vec model on the given dataset whereas the google model was trained on a different dataset. Yet, the google model does perform decently well in comparison to our model. We should be able to see an improvement in the performance of both the word2vec features when a model that can handle complex dependencies is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bfd6f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm = LinearSVC(dual = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0b95ce9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of SVM model trained using Google Word2Vec features: \n",
      "Testing Accuracy:  0.817625\n",
      "Testing Precision:  0.8347024749868351\n",
      "Testing Recall:  0.7922727045534063\n",
      "Testing F1 Score:  0.8129343282816626\n"
     ]
    }
   ],
   "source": [
    "#SVM model trained using Google Word2Vec features\n",
    "svm.fit(x_google_train, y_google_train)\n",
    "y_pred_train = svm.predict(x_google_train)\n",
    "y_pred_test = svm.predict(x_google_test)\n",
    "print(\"Statistics of SVM model trained using Google Word2Vec features: \")\n",
    "print(\"Testing Accuracy: \", accuracy_score(y_google_test, y_pred_test))\n",
    "print(\"Testing Precision: \", precision_score(y_google_test, y_pred_test))\n",
    "print(\"Testing Recall: \", recall_score(y_google_test, y_pred_test))\n",
    "print(\"Testing F1 Score: \", f1_score(y_google_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1a756912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of SVM model trained using Google Word2Vec features: \n",
      "Testing Accuracy:  0.83989199459973\n",
      "Testing Precision:  0.8487951500331142\n",
      "Testing Recall:  0.8290292083395532\n",
      "Testing F1 Score:  0.8387957508936214\n"
     ]
    }
   ],
   "source": [
    "#SVM model trained using Amazon Word2Vec features\n",
    "svm.fit(x_amazon_train, y_amazon_train)\n",
    "y_pred_train = svm.predict(x_amazon_train)\n",
    "y_pred_test = svm.predict(x_amazon_test)\n",
    "print(\"Statistics of SVM model trained using Google Word2Vec features: \")\n",
    "print(\"Testing Accuracy: \", accuracy_score(y_amazon_test, y_pred_test))\n",
    "print(\"Testing Precision: \", precision_score(y_amazon_test, y_pred_test))\n",
    "print(\"Testing Recall: \", recall_score(y_amazon_test, y_pred_test))\n",
    "print(\"Testing F1 Score: \", f1_score(y_amazon_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ccba4318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of SVM model trained using TF_IDF features: \n",
      "Testing Accuracy:  0.8721\n",
      "Testing Precision:  0.8736012845601887\n",
      "Testing Recall:  0.8701954315989404\n",
      "Testing F1 Score:  0.8718950320512822\n"
     ]
    }
   ],
   "source": [
    "#SVM model trained using TF_IDF features\n",
    "svm.fit(x_tfidf_train, y_tfidf_train)\n",
    "y_pred_train = svm.predict(x_tfidf_train)\n",
    "y_pred_test = svm.predict(x_tfidf_test)\n",
    "print(\"Statistics of SVM model trained using TF_IDF features: \")\n",
    "print(\"Testing Accuracy: \", accuracy_score(y_tfidf_test, y_pred_test))\n",
    "print(\"Testing Precision: \", precision_score(y_tfidf_test, y_pred_test))\n",
    "print(\"Testing Recall: \", recall_score(y_tfidf_test, y_pred_test))\n",
    "print(\"Testing F1 Score: \", f1_score(y_tfidf_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e46397",
   "metadata": {},
   "source": [
    "As mentioned above the SVM, a more complex model than the perceptron, produces much better results for the google word2vec features as well as the amazon reviews word2vec features. The TF_IDF features also produce good results which are in fact better than those produced by the word2vec features. From these results we can infer that, the SVM is better at capturing the complex dependencies between the word vectors and the corresponding labels, the gap in the performance between TF_IDF features model and the word2vec features model shows that the SVM isn't complex enough. We can also note that the SVM trained with our word2vec model features still outperforms the one trained with google word2vec model feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a9872",
   "metadata": {},
   "source": [
    "### Feed Forward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0b1ed1",
   "metadata": {},
   "source": [
    "Preparing data for the ternary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "aa2341b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_google_ternary = google_df.drop(['label'], axis = 1).values\n",
    "y_google_ternary = google_df.label.values\n",
    "x_amazon_ternary = amazon_df.drop(['label'], axis = 1).values\n",
    "y_amazon_ternary = amazon_df.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccef46a",
   "metadata": {},
   "source": [
    "Train_Test Split for ternary classification dataset. 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e0aefa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_google_ternary_train, x_google_ternary_test, y_google_ternary_train, y_google_ternary_test = train_test_split(x_google_ternary, y_google_ternary, test_size = 0.2, random_state = 42)\n",
    "x_amazon_ternary_train, x_amazon_ternary_test, y_amazon_ternary_train, y_amazon_ternary_test = train_test_split(x_amazon_ternary, y_amazon_ternary, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a784e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as func\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70e9c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pynvml\n",
    "# def get_memory_free_MiB(gpu_index):\n",
    "#     pynvml.nvmlInit()\n",
    "#     handle = pynvml.nvmlDeviceGetHandleByIndex(int(gpu_index))\n",
    "#     mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "#     return mem_info.free // 1024 ** 2\n",
    "# get_memory_free_MiB(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c7c4e",
   "metadata": {},
   "source": [
    "Creating validation set. In total, 80/20/20 split for training, validation and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5f403e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_google_train, x_google_val, y_google_train, y_google_val = train_test_split(x_google_train, y_google_train, test_size = 0.25, random_state = 42)\n",
    "x_amazon_train, x_amazon_val, y_amazon_train, y_amazon_val = train_test_split(x_amazon_train, y_amazon_train, test_size = 0.25, random_state = 42)\n",
    "\n",
    "x_google_ternary_train, x_google_ternary_val, y_google_ternary_train, y_google_ternary_val = train_test_split(x_google_ternary_train, y_google_ternary_train, test_size = 0.25, random_state = 42)\n",
    "x_amazon_ternary_train, x_amazon_ternary_val, y_amazon_ternary_train, y_amazon_ternary_val = train_test_split(x_amazon_ternary_train, y_amazon_ternary_train, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195e022c",
   "metadata": {},
   "source": [
    "Changing the numpy n-dimensional arrays to torch float tensors and torch long tensors and assigning them to \"device\" which would be the GPU (cuda core) if it is available, else it will be the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dd2119d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_google_train, x_google_val, x_google_test, y_google_train, y_google_val, y_google_test = torch.FloatTensor(x_google_train).to(device), torch.FloatTensor(x_google_val).to(device), torch.FloatTensor(x_google_test).to(device), torch.LongTensor(y_google_train).to(device), torch.LongTensor(y_google_val).to(device), torch.LongTensor(y_google_test).to(device)\n",
    "x_amazon_train, x_amazon_val, x_amazon_test, y_amazon_train, y_amazon_val, y_amazon_test = torch.FloatTensor(x_amazon_train), torch.FloatTensor(x_amazon_val).to(device), torch.FloatTensor(x_amazon_test).to(device), torch.LongTensor(y_amazon_train).to(device), torch.LongTensor(y_amazon_val).to(device), torch.LongTensor(y_amazon_test).to(device)\n",
    "x_google_ternary_train, x_google_ternary_val, x_google_ternary_test, y_google_ternary_train, y_google_ternary_val, y_google_ternary_test = torch.FloatTensor(x_google_ternary_train).to(device), torch.FloatTensor(x_google_ternary_val).to(device), torch.FloatTensor(x_google_ternary_test).to(device), torch.LongTensor(y_google_ternary_train).to(device), torch.LongTensor(y_google_ternary_val).to(device), torch.LongTensor(y_google_ternary_test).to(device)\n",
    "x_amazon_ternary_train, x_amazon_ternary_val, x_amazon_ternary_test, y_amazon_ternary_train, y_amazon_ternary_val, y_amazon_ternary_test = torch.FloatTensor(x_amazon_ternary_train).to(device), torch.FloatTensor(x_amazon_ternary_val).to(device), torch.FloatTensor(x_amazon_ternary_test).to(device), torch.LongTensor(y_amazon_ternary_train).to(device), torch.LongTensor(y_amazon_ternary_val).to(device), torch.LongTensor(y_amazon_ternary_test).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e61e5f",
   "metadata": {},
   "source": [
    "Creating the dataset class for our dataset that will be used by the DataLoader function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fc774cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])\n",
    "\n",
    "class data(Dataset):\n",
    "    def __init__(self, inputs, transform = None):\n",
    "        self.data = inputs\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.data[index][0]\n",
    "        label = self.data[index][1]\n",
    "        if self.transform is not None:\n",
    "            inputs = self.transform(inputs)\n",
    "            \n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1459e6",
   "metadata": {},
   "source": [
    "Combining the features and labels into one tensor dataset and then changing their type to be the type of the dataset class that we have defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c64f2974",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "google_train_dataset = TensorDataset(x_google_train, y_google_train)\n",
    "google_train_dataset = data(google_train_dataset)\n",
    "google_val_dataset = TensorDataset(x_google_val, y_google_val)\n",
    "google_val_dataset = data(google_val_dataset)\n",
    "google_test_dataset = TensorDataset(x_google_test, y_google_test)\n",
    "google_test_dataset = data(google_test_dataset)\n",
    "\n",
    "google_train_loader = DataLoader(google_train_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "google_val_loader = DataLoader(google_val_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "google_test_loader = DataLoader(google_test_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "\n",
    "google_ternary_train_dataset = TensorDataset(x_google_ternary_train, y_google_ternary_train)\n",
    "google_ternary_train_dataset = data(google_ternary_train_dataset)\n",
    "google_ternary_val_dataset = TensorDataset(x_google_ternary_val, y_google_ternary_val)\n",
    "google_ternary_val_dataset = data(google_ternary_val_dataset)\n",
    "google_ternary_test_dataset = TensorDataset(x_google_ternary_test, y_google_ternary_test)\n",
    "google_ternary_test_dataset = data(google_ternary_test_dataset)\n",
    "\n",
    "google_ternary_train_loader = DataLoader(google_ternary_train_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "google_ternary_val_loader = DataLoader(google_ternary_val_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "google_ternary_test_loader = DataLoader(google_ternary_test_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "\n",
    "amazon_train_dataset = TensorDataset(x_amazon_train, y_amazon_train)\n",
    "amazon_train_dataset = data(amazon_train_dataset)\n",
    "amazon_val_dataset = TensorDataset(x_amazon_val, y_amazon_val)\n",
    "amazon_val_dataset = data(amazon_val_dataset)\n",
    "amazon_test_dataset = TensorDataset(x_amazon_test, y_amazon_test)\n",
    "amazon_test_dataset = data(amazon_test_dataset)\n",
    "\n",
    "amazon_train_loader = DataLoader(amazon_train_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "amazon_val_loader = DataLoader(amazon_val_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "amazon_test_loader = DataLoader(amazon_test_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "\n",
    "amazon_ternary_train_dataset = TensorDataset(x_amazon_ternary_train, y_amazon_ternary_train)\n",
    "amazon_ternary_train_dataset = data(amazon_ternary_train_dataset)\n",
    "amazon_ternary_val_dataset = TensorDataset(x_amazon_ternary_val, y_amazon_ternary_val)\n",
    "amazon_ternary_val_dataset = data(amazon_ternary_val_dataset)\n",
    "amazon_ternary_test_dataset = TensorDataset(x_amazon_ternary_test, y_amazon_ternary_test)\n",
    "amazon_ternary_test_dataset = data(amazon_ternary_test_dataset)\n",
    "\n",
    "amazon_ternary_train_loader = DataLoader(amazon_ternary_train_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "amazon_ternary_val_loader = DataLoader(amazon_ternary_val_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "amazon_ternary_test_loader = DataLoader(amazon_ternary_test_dataset, batch_size = batch_size, drop_last = True, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdaf324",
   "metadata": {},
   "source": [
    "Initializing the feed forward neural network model for the binary classification task. Hyperparameters, learning_rate = 0.03, dropout = 0.2 and SGD optimizer have been used across all the average vector Feed-Forward network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "13cda549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff_nn_binary(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ff_nn_binary(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ff_nn_binary, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        self.fc1 = torch.nn.Linear(300, hidden_1)\n",
    "        self.fc2 = torch.nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = torch.nn.Linear(hidden_2, 1)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = func.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = func.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = ff_nn_binary().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfb7b11",
   "metadata": {},
   "source": [
    "Initializing the SGD optimizer with the model parameters also initialising the Binary Cross Entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "57882124",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e39d6",
   "metadata": {},
   "source": [
    "Training the neural network model using the google word2vec features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f0e7a48f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.034653 \tValidation Loss: 0.034625\n",
      "Validation loss decreased (inf --> 0.034625). Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.034518 \tValidation Loss: 0.034131\n",
      "Validation loss decreased (0.034625 --> 0.034131). Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.031292 \tValidation Loss: 0.025858\n",
      "Validation loss decreased (0.034131 --> 0.025858). Saving model...\n",
      "Epoch: 4 \tTraining Loss: 0.026738 \tValidation Loss: 0.023430\n",
      "Validation loss decreased (0.025858 --> 0.023430). Saving model...\n",
      "Epoch: 5 \tTraining Loss: 0.025417 \tValidation Loss: 0.022585\n",
      "Validation loss decreased (0.023430 --> 0.022585). Saving model...\n",
      "Epoch: 6 \tTraining Loss: 0.024582 \tValidation Loss: 0.023827\n",
      "Epoch: 7 \tTraining Loss: 0.024073 \tValidation Loss: 0.022912\n",
      "Epoch: 8 \tTraining Loss: 0.023661 \tValidation Loss: 0.021227\n",
      "Validation loss decreased (0.022585 --> 0.021227). Saving model...\n",
      "Epoch: 9 \tTraining Loss: 0.023488 \tValidation Loss: 0.023560\n",
      "Epoch: 10 \tTraining Loss: 0.023277 \tValidation Loss: 0.021997\n",
      "Epoch: 11 \tTraining Loss: 0.023119 \tValidation Loss: 0.020754\n",
      "Validation loss decreased (0.021227 --> 0.020754). Saving model...\n",
      "Epoch: 12 \tTraining Loss: 0.022905 \tValidation Loss: 0.021569\n",
      "Epoch: 13 \tTraining Loss: 0.022765 \tValidation Loss: 0.021224\n",
      "Epoch: 14 \tTraining Loss: 0.022579 \tValidation Loss: 0.020517\n",
      "Validation loss decreased (0.020754 --> 0.020517). Saving model...\n",
      "Epoch: 15 \tTraining Loss: 0.022364 \tValidation Loss: 0.020657\n",
      "Epoch: 16 \tTraining Loss: 0.022298 \tValidation Loss: 0.020652\n",
      "Epoch: 17 \tTraining Loss: 0.022285 \tValidation Loss: 0.020521\n",
      "Epoch: 18 \tTraining Loss: 0.022162 \tValidation Loss: 0.020530\n",
      "Epoch: 19 \tTraining Loss: 0.022100 \tValidation Loss: 0.020445\n",
      "Validation loss decreased (0.020517 --> 0.020445). Saving model...\n",
      "Epoch: 20 \tTraining Loss: 0.022022 \tValidation Loss: 0.020669\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "valid_min_loss = np.inf\n",
    "train_loader = google_train_loader\n",
    "val_loader = google_val_loader\n",
    "test_loader = google_test_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1), target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    for inputs, target in val_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1), target.float())\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(val_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss,valid_loss))\n",
    "    if valid_loss <= valid_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_min_loss, valid_loss))\n",
    "        torch.save(model.state_dict(), 'model/google_model.pt')\n",
    "        valid_min_loss = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b17c606",
   "metadata": {},
   "source": [
    "Testing the model on the test dataset. And as you can see, the google model's accuracy has gone up indicating that the feed-forward neural network is indeed a more complex model than perceptron and SVM and is able to capture the underlying relationship between the average word vector and the label better. The accuracy is still isn't vastly better, and this may be due to two reasons. One, the model still isn't complex enough or two, the svm and perceptron train for 1000 iterations (maximum) and the FFNN (feed-forward neural network) just trains for 20 epochs thus limiting the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7bb37e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of Google Word2Vec features for binary classification model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.82     19993\n",
      "           1       0.85      0.76      0.81     20007\n",
      "\n",
      "    accuracy                           0.82     40000\n",
      "   macro avg       0.82      0.82      0.82     40000\n",
      "weighted avg       0.82      0.82      0.82     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "test_loader = google_test_loader\n",
    "model = ff_nn_binary().to(device)\n",
    "model.load_state_dict(torch.load('model/google_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        y_test_pred = model(inputs)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_targ_list.append(target.cpu().numpy())\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "print(\"Statistics of Google Word2Vec features for binary classification model\")\n",
    "print(classification_report(y_targ_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff2e00",
   "metadata": {},
   "source": [
    "Re-initialising the model and the optimizer so that the weights, gradients and other model parameters aren't carried over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "665b426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del optimizer\n",
    "del loss_fn\n",
    "model = ff_nn_binary().to(device)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad40a3f",
   "metadata": {},
   "source": [
    "Training the neural network model using the amazon review word2vec features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4a241ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.024246 \tValidation Loss: 0.019032\n",
      "Validation loss decreased (inf --> 0.019032). Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.019844 \tValidation Loss: 0.018640\n",
      "Validation loss decreased (0.019032 --> 0.018640). Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.019228 \tValidation Loss: 0.017923\n",
      "Validation loss decreased (0.018640 --> 0.017923). Saving model...\n",
      "Epoch: 4 \tTraining Loss: 0.018770 \tValidation Loss: 0.017490\n",
      "Validation loss decreased (0.017923 --> 0.017490). Saving model...\n",
      "Epoch: 5 \tTraining Loss: 0.018378 \tValidation Loss: 0.017336\n",
      "Validation loss decreased (0.017490 --> 0.017336). Saving model...\n",
      "Epoch: 6 \tTraining Loss: 0.018105 \tValidation Loss: 0.017187\n",
      "Validation loss decreased (0.017336 --> 0.017187). Saving model...\n",
      "Epoch: 7 \tTraining Loss: 0.017803 \tValidation Loss: 0.017401\n",
      "Epoch: 8 \tTraining Loss: 0.017596 \tValidation Loss: 0.016839\n",
      "Validation loss decreased (0.017187 --> 0.016839). Saving model...\n",
      "Epoch: 9 \tTraining Loss: 0.017475 \tValidation Loss: 0.016836\n",
      "Validation loss decreased (0.016839 --> 0.016836). Saving model...\n",
      "Epoch: 10 \tTraining Loss: 0.017332 \tValidation Loss: 0.016950\n",
      "Epoch: 11 \tTraining Loss: 0.017187 \tValidation Loss: 0.017222\n",
      "Epoch: 12 \tTraining Loss: 0.017078 \tValidation Loss: 0.016472\n",
      "Validation loss decreased (0.016836 --> 0.016472). Saving model...\n",
      "Epoch: 13 \tTraining Loss: 0.016999 \tValidation Loss: 0.016686\n",
      "Epoch: 14 \tTraining Loss: 0.016859 \tValidation Loss: 0.016587\n",
      "Epoch: 15 \tTraining Loss: 0.016816 \tValidation Loss: 0.016611\n",
      "Epoch: 16 \tTraining Loss: 0.016730 \tValidation Loss: 0.016375\n",
      "Validation loss decreased (0.016472 --> 0.016375). Saving model...\n",
      "Epoch: 17 \tTraining Loss: 0.016674 \tValidation Loss: 0.016325\n",
      "Validation loss decreased (0.016375 --> 0.016325). Saving model...\n",
      "Epoch: 18 \tTraining Loss: 0.016607 \tValidation Loss: 0.016342\n",
      "Epoch: 19 \tTraining Loss: 0.016473 \tValidation Loss: 0.016406\n",
      "Epoch: 20 \tTraining Loss: 0.016408 \tValidation Loss: 0.016404\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "valid_min_loss = np.inf\n",
    "train_loader = amazon_train_loader\n",
    "val_loader = amazon_val_loader\n",
    "test_loader = amazon_test_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1), target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    for inputs, target in val_loader:\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1), target.float())\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(val_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss,valid_loss))\n",
    "    if valid_loss <= valid_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_min_loss, valid_loss))\n",
    "        torch.save(model.state_dict(), 'model/amazon_model.pt')\n",
    "        valid_min_loss = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc6766",
   "metadata": {},
   "source": [
    "Testing the model, and once again as you can see from below, the amazon word2vec feature model outperforms its counterpart. And, as noted in the google word2vec feature model, the FFNN for amazon word2vec features perform better than the simple models (SVM, Perceptron)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "44d9dc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of Amazon Word2Vec features for binary classification model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.86     19892\n",
      "           1       0.86      0.86      0.86     20088\n",
      "\n",
      "    accuracy                           0.86     39980\n",
      "   macro avg       0.86      0.86      0.86     39980\n",
      "weighted avg       0.86      0.86      0.86     39980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "test_loader = amazon_test_loader\n",
    "model = ff_nn_binary().to(device)\n",
    "model.load_state_dict(torch.load('model/amazon_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        y_test_pred = model(inputs)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_targ_list.append(target.cpu().numpy())\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "print(\"Statistics of Amazon Word2Vec features for binary classification model\")\n",
    "print(classification_report(y_targ_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e054a9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff_nn_ternary(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ff_nn_ternary(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ff_nn_ternary, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        self.fc1 = torch.nn.Linear(300, hidden_1)\n",
    "        self.fc2 = torch.nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = torch.nn.Linear(hidden_2, 3)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = func.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = func.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = ff_nn_ternary()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a94ecff",
   "metadata": {},
   "source": [
    "Since the dataset is skewed towards two classes (positive ~100k samples and negative ~100k samples, neutral ~50k samples), we have to include class weights so that the model could be penalized more for making a mistake in predicting the smaller sized class which ultimately leads to better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6c26c8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights of ternary classification for Google word2vec feature model:\n",
      "[0.83333333 0.83333333 1.66666667]\n",
      "\n",
      "Class weights of ternary classification for Amazon word2vec feature model:\n",
      "[0.83329833 0.83333167 1.66681336]\n"
     ]
    }
   ],
   "source": [
    "np_ary = y_google_ternary\n",
    "class_weights_google = class_weight.compute_class_weight('balanced', np.unique(np_ary), np_ary)\n",
    "print(\"Class weights of ternary classification for Google word2vec feature model:\")\n",
    "print(class_weights_google)\n",
    "np_ary = y_amazon_ternary\n",
    "class_weights_amazon = class_weight.compute_class_weight('balanced', np.unique(np_ary), np_ary)\n",
    "print(\"\\nClass weights of ternary classification for Amazon word2vec feature model:\")\n",
    "print(class_weights_amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fc00c935",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del optimizer\n",
    "del loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "690a071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ff_nn_ternary().to(device)\n",
    "class_weights = torch.FloatTensor(class_weights_google)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight = class_weights).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3914692a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.054932 \tValidation Loss: 0.054913\n",
      "Validation loss decreased (inf --> 0.054913). Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.054841 \tValidation Loss: 0.054534\n",
      "Validation loss decreased (0.054913 --> 0.054534). Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.052012 \tValidation Loss: 0.047934\n",
      "Validation loss decreased (0.054534 --> 0.047934). Saving model...\n",
      "Epoch: 4 \tTraining Loss: 0.048529 \tValidation Loss: 0.045973\n",
      "Validation loss decreased (0.047934 --> 0.045973). Saving model...\n",
      "Epoch: 5 \tTraining Loss: 0.047422 \tValidation Loss: 0.045821\n",
      "Validation loss decreased (0.045973 --> 0.045821). Saving model...\n",
      "Epoch: 6 \tTraining Loss: 0.046736 \tValidation Loss: 0.044438\n",
      "Validation loss decreased (0.045821 --> 0.044438). Saving model...\n",
      "Epoch: 7 \tTraining Loss: 0.046318 \tValidation Loss: 0.045213\n",
      "Epoch: 8 \tTraining Loss: 0.045980 \tValidation Loss: 0.043992\n",
      "Validation loss decreased (0.044438 --> 0.043992). Saving model...\n",
      "Epoch: 9 \tTraining Loss: 0.045614 \tValidation Loss: 0.044088\n",
      "Epoch: 10 \tTraining Loss: 0.045478 \tValidation Loss: 0.043552\n",
      "Validation loss decreased (0.043992 --> 0.043552). Saving model...\n",
      "Epoch: 11 \tTraining Loss: 0.045293 \tValidation Loss: 0.043627\n",
      "Epoch: 12 \tTraining Loss: 0.045121 \tValidation Loss: 0.044053\n",
      "Epoch: 13 \tTraining Loss: 0.045079 \tValidation Loss: 0.043757\n",
      "Epoch: 14 \tTraining Loss: 0.045016 \tValidation Loss: 0.044425\n",
      "Epoch: 15 \tTraining Loss: 0.044807 \tValidation Loss: 0.043790\n",
      "Epoch: 16 \tTraining Loss: 0.044768 \tValidation Loss: 0.043518\n",
      "Validation loss decreased (0.043552 --> 0.043518). Saving model...\n",
      "Epoch: 17 \tTraining Loss: 0.044732 \tValidation Loss: 0.043436\n",
      "Validation loss decreased (0.043518 --> 0.043436). Saving model...\n",
      "Epoch: 18 \tTraining Loss: 0.044615 \tValidation Loss: 0.043415\n",
      "Validation loss decreased (0.043436 --> 0.043415). Saving model...\n",
      "Epoch: 19 \tTraining Loss: 0.044609 \tValidation Loss: 0.042931\n",
      "Validation loss decreased (0.043415 --> 0.042931). Saving model...\n",
      "Epoch: 20 \tTraining Loss: 0.044519 \tValidation Loss: 0.042890\n",
      "Validation loss decreased (0.042931 --> 0.042890). Saving model...\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "valid_min_loss = np.inf\n",
    "train_loader = google_ternary_train_loader\n",
    "val_loader = google_ternary_val_loader\n",
    "test_loader = google_ternary_test_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    for inputs, target in val_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, target)\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(val_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss, valid_loss))\n",
    "    if valid_loss <= valid_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_min_loss, valid_loss))\n",
    "        torch.save(model.state_dict(), 'model/google_ternary_model.pt')\n",
    "        valid_min_loss = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17208a6f",
   "metadata": {},
   "source": [
    "As you can see from the below statistics the model performs significantly worse on ternary classification when compared to the binary classification task. This again, could be due to two reasons. One, the dataset isn't proper. Specifically, the neutral class has to be pretty ambiguous to produce such results. By ambiguous, I mean that the neutral reviews aren't totally neutral and they are either somewhat positive or somewhat negative. The ambiguity of the neutral class affects the precision and recall of both the negative and positive classes thus bringing the whole performance of the model down. Two, the relation between the vectors and the labels with the addition of the neutral class has become more complex such that the model is no longer able to represent that relation effectively. This could be further corroborated by the training loss which is pretty high even after 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9cd6743c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of Google Word2Vec features for ternary classification model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.68      0.70     19850\n",
      "           1       0.76      0.68      0.72     20114\n",
      "           2       0.35      0.46      0.40     10036\n",
      "\n",
      "    accuracy                           0.64     50000\n",
      "   macro avg       0.61      0.61      0.61     50000\n",
      "weighted avg       0.66      0.64      0.65     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "test_loader = google_ternary_test_loader\n",
    "model = ff_nn_ternary().to(device)\n",
    "model.load_state_dict(torch.load('model/google_ternary_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        y_test_pred = model(inputs)\n",
    "        _, y_test_pred = torch.max(y_test_pred, 1)\n",
    "        y_pred_tag = y_test_pred\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_targ_list.append(target.cpu().numpy())\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "print(\"Statistics of Google Word2Vec features for ternary classification model\")\n",
    "print(classification_report(y_targ_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8ea294c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del optimizer\n",
    "del loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "92ddb03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ff_nn_ternary().to(device)\n",
    "class_weights = torch.FloatTensor(class_weights_amazon)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight = class_weights).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bd998d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.047069 \tValidation Loss: 0.041461\n",
      "Validation loss decreased (inf --> 0.041461). Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.042255 \tValidation Loss: 0.040320\n",
      "Validation loss decreased (0.041461 --> 0.040320). Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.041527 \tValidation Loss: 0.039922\n",
      "Validation loss decreased (0.040320 --> 0.039922). Saving model...\n",
      "Epoch: 4 \tTraining Loss: 0.041095 \tValidation Loss: 0.039999\n",
      "Epoch: 5 \tTraining Loss: 0.040745 \tValidation Loss: 0.039532\n",
      "Validation loss decreased (0.039922 --> 0.039532). Saving model...\n",
      "Epoch: 6 \tTraining Loss: 0.040532 \tValidation Loss: 0.039621\n",
      "Epoch: 7 \tTraining Loss: 0.040253 \tValidation Loss: 0.039001\n",
      "Validation loss decreased (0.039532 --> 0.039001). Saving model...\n",
      "Epoch: 8 \tTraining Loss: 0.040066 \tValidation Loss: 0.039078\n",
      "Epoch: 9 \tTraining Loss: 0.039973 \tValidation Loss: 0.038908\n",
      "Validation loss decreased (0.039001 --> 0.038908). Saving model...\n",
      "Epoch: 10 \tTraining Loss: 0.039891 \tValidation Loss: 0.038728\n",
      "Validation loss decreased (0.038908 --> 0.038728). Saving model...\n",
      "Epoch: 11 \tTraining Loss: 0.039705 \tValidation Loss: 0.038973\n",
      "Epoch: 12 \tTraining Loss: 0.039645 \tValidation Loss: 0.038655\n",
      "Validation loss decreased (0.038728 --> 0.038655). Saving model...\n",
      "Epoch: 13 \tTraining Loss: 0.039488 \tValidation Loss: 0.039085\n",
      "Epoch: 14 \tTraining Loss: 0.039456 \tValidation Loss: 0.038532\n",
      "Validation loss decreased (0.038655 --> 0.038532). Saving model...\n",
      "Epoch: 15 \tTraining Loss: 0.039349 \tValidation Loss: 0.038617\n",
      "Epoch: 16 \tTraining Loss: 0.039324 \tValidation Loss: 0.038427\n",
      "Validation loss decreased (0.038532 --> 0.038427). Saving model...\n",
      "Epoch: 17 \tTraining Loss: 0.039278 \tValidation Loss: 0.038505\n",
      "Epoch: 18 \tTraining Loss: 0.039155 \tValidation Loss: 0.038379\n",
      "Validation loss decreased (0.038427 --> 0.038379). Saving model...\n",
      "Epoch: 19 \tTraining Loss: 0.039132 \tValidation Loss: 0.038647\n",
      "Epoch: 20 \tTraining Loss: 0.038993 \tValidation Loss: 0.038509\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "valid_min_loss = np.inf\n",
    "train_loader = amazon_ternary_train_loader\n",
    "val_loader = amazon_ternary_val_loader\n",
    "test_loader = amazon_ternary_test_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "#         target = target.squeeze(1)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    for inputs, target in val_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        output = model(inputs)\n",
    "#         target = target.squeeze(1)\n",
    "        loss = loss_fn(output, target)\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(val_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss, valid_loss))\n",
    "    if valid_loss <= valid_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_min_loss, valid_loss))\n",
    "        torch.save(model.state_dict(), 'model/amazon_ternary_model.pt')\n",
    "        valid_min_loss = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89848a0",
   "metadata": {},
   "source": [
    "Once again, the amazon word2vec feature model produces better results than the google word2vec feature model. Still, the preformance is pretty mediocre in comparison to the binary classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c63cb73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of Google Word2Vec features for ternary classification model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.69      0.73     19834\n",
      "           1       0.80      0.72      0.76     20219\n",
      "           2       0.37      0.54      0.44      9927\n",
      "\n",
      "    accuracy                           0.67     49980\n",
      "   macro avg       0.65      0.65      0.64     49980\n",
      "weighted avg       0.71      0.67      0.68     49980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "test_loader = amazon_ternary_test_loader\n",
    "model = ff_nn_ternary().to(device)\n",
    "model.load_state_dict(torch.load('model/amazon_ternary_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        y_test_pred = model(inputs)\n",
    "        _, y_test_pred = torch.max(y_test_pred, 1)\n",
    "        y_pred_tag = y_test_pred\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_targ_list.append(target.cpu().numpy())\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "print(\"Statistics of Amazon Word2Vec features for ternary classification model\")\n",
    "print(classification_report(y_targ_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e1cd29",
   "metadata": {},
   "source": [
    "## FFNN with word vectors of first 10 words of the review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7a913",
   "metadata": {},
   "source": [
    "Below is the function that calculates the vector representations of the first 10 words of the review and returns a np.ndarray. We ignore words that aren't present in a model and if a review doesn't have any words or if none of the words in the review are present in the specified model, we return a nan value which would be removed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e609eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_10_w2v(sentence, model):\n",
    "    result_vector = np.zeros(3000, dtype = np.float64)\n",
    "    count = 0\n",
    "    split_list = sentence.split(\" \")\n",
    "    len_list = len(split_list)\n",
    "    if model == 'google':\n",
    "        for word in split_list:\n",
    "            if count == 10 or count == len_list:\n",
    "                break\n",
    "            count += 1\n",
    "            try:\n",
    "                result_vector[(count - 1) * 300 : count * 300] = w2v_google[word]\n",
    "            except KeyError:\n",
    "                count -= 1\n",
    "                continue\n",
    "    else:\n",
    "        for word in split_list:\n",
    "            if count == 10 or count == len_list:\n",
    "                break\n",
    "            count += 1\n",
    "            try:\n",
    "                result_vector[(count - 1) * 300 : count * 300] = w2v_model.wv[word]\n",
    "            except KeyError:\n",
    "                count -= 1\n",
    "                continue\n",
    "    \n",
    "    if np.all((result_vector == 0)):    \n",
    "        return np.nan\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249809b3",
   "metadata": {},
   "source": [
    "We store the vector and the label as a list of tuples, as we previously did, and then we remove nan values. Later, we store it as a pickle file (reading and writing a pickle file for such a big dataset proved to be much faster when compared to csv file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "401db2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_f10 = [(compute_10_w2v(df_new.iloc[[x]].review_body.values[0], \"google\"), df_new.iloc[[x]].star_rating.values[0]) for x in range(len(df_new))]\n",
    "google_f10 = [x for x in google_f10 if np.isnan(x[0]).any() == False]\n",
    "with open(\"google_f10.pkl\", \"wb\") as file_obj:\n",
    "    pkl.dump(google_f10, file_obj)\n",
    "del google_f10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f67bb157",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_f10 = [(compute_10_w2v(df_new.iloc[[x]].review_body.values[0], \"google\"), df_new.iloc[[x]].star_rating.values[0]) for x in range(len(df_new))]\n",
    "amazon_f10 = [x for x in amazon_f10 if np.isnan(x[0]).any() == False]\n",
    "with open(\"amazon_f10.pkl\", \"wb\") as file_obj:\n",
    "    pkl.dump(amazon_f10, file_obj)\n",
    "del amazon_f10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbae920",
   "metadata": {},
   "source": [
    "##### Checkpoint - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e2ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/google_f10.pkl\", \"rb\") as file_obj:\n",
    "    google_f10 = pkl.load(file_obj)\n",
    "    \n",
    "with open(\"data/amazon_f10.pkl\", \"rb\") as file_obj:\n",
    "    amazon_f10 = pkl.load(file_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532449e4",
   "metadata": {},
   "source": [
    "Now, we create the binary dataset from the original dataset and then separate them into input(x) and output(y) for both binary and ternary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a21f8ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_f10_binary = [x for x in google_f10 if x[1] == 0 or x[1] == 1]\n",
    "amazon_f10_binary = [x for x in amazon_f10 if x[1] == 0 or x[1] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "892d8ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_google_f10 = np.array([x[0] for x in google_f10_binary])\n",
    "y_google_f10 = np.array([x[1] for x in google_f10_binary])\n",
    "\n",
    "x_amazon_f10 = np.array([x[0] for x in amazon_f10_binary])\n",
    "y_amazon_f10 = np.array([x[1] for x in amazon_f10_binary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2561f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_google_f10_ternary = np.array([x[0] for x in google_f10])\n",
    "y_google_f10_ternary = np.array([x[1] for x in google_f10])\n",
    "\n",
    "x_amazon_f10_ternary = np.array([x[0] for x in amazon_f10])\n",
    "y_amazon_f10_ternary = np.array([x[1] for x in amazon_f10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed17bbf",
   "metadata": {},
   "source": [
    "Then, we split the data into train and test. 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4a4469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_google_f10_train, x_google_f10_test, y_google_f10_train, y_google_f10_test = train_test_split(x_google_f10, y_google_f10, test_size=0.2, random_state=42)\n",
    "x_amazon_f10_train, x_amazon_f10_test, y_amazon_f10_train, y_amazon_f10_test = train_test_split(x_amazon_f10, y_amazon_f10, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5816f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_google_f10_ternary_train, x_google_f10_ternary_test, y_google_f10_ternary_train, y_google_f10_ternary_test = train_test_split(x_google_f10_ternary, y_google_f10_ternary, test_size = 0.2, random_state = 42)\n",
    "x_amazon_f10_ternary_train, x_amazon_f10_ternary_test, y_amazon_f10_ternary_train, y_amazon_f10_ternary_test = train_test_split(x_amazon_f10_ternary, y_amazon_f10_ternary, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852d457e",
   "metadata": {},
   "source": [
    "80/20/20 - train/val/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80d5aa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_google_f10_train, x_google_f10_val, y_google_f10_train, y_google_f10_val = train_test_split(x_google_f10_train, y_google_f10_train, test_size = 0.25, random_state = 42)\n",
    "x_amazon_f10_train, x_amazon_f10_val, y_amazon_f10_train, y_amazon_f10_val = train_test_split(x_amazon_f10_train, y_amazon_f10_train, test_size = 0.25, random_state = 42)\n",
    "\n",
    "x_google_f10_ternary_train, x_google_f10_ternary_val, y_google_f10_ternary_train, y_google_f10_ternary_val = train_test_split(x_google_f10_ternary_train, y_google_f10_ternary_train, test_size = 0.25, random_state = 42)\n",
    "x_amazon_f10_ternary_train, x_amazon_f10_ternary_val, y_amazon_f10_ternary_train, y_amazon_f10_ternary_val = train_test_split(x_amazon_f10_ternary_train, y_amazon_f10_ternary_train, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5b1439",
   "metadata": {},
   "source": [
    "Converting the arrays into float and long tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81df57d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_google_f10_train, x_google_f10_val, x_google_f10_test, y_google_f10_train, y_google_f10_val, y_google_f10_test = torch.FloatTensor(x_google_f10_train), torch.FloatTensor(x_google_f10_val), torch.FloatTensor(x_google_f10_test), torch.LongTensor(y_google_f10_train), torch.LongTensor(y_google_f10_val), torch.LongTensor(y_google_f10_test)\n",
    "x_amazon_f10_train, x_amazon_f10_val, x_amazon_f10_test, y_amazon_f10_train, y_amazon_f10_val, y_amazon_f10_test = torch.FloatTensor(x_amazon_f10_train), torch.FloatTensor(x_amazon_f10_val), torch.FloatTensor(x_amazon_f10_test), torch.LongTensor(y_amazon_f10_train), torch.LongTensor(y_amazon_f10_val), torch.LongTensor(y_amazon_f10_test)\n",
    "x_google_f10_ternary_train, x_google_f10_ternary_val, x_google_f10_ternary_test, y_google_f10_ternary_train, y_google_f10_ternary_val, y_google_f10_ternary_test = torch.FloatTensor(x_google_f10_ternary_train), torch.FloatTensor(x_google_f10_ternary_val), torch.FloatTensor(x_google_f10_ternary_test), torch.LongTensor(y_google_f10_ternary_train), torch.LongTensor(y_google_f10_ternary_val), torch.LongTensor(y_google_f10_ternary_test)\n",
    "x_amazon_f10_ternary_train, x_amazon_f10_ternary_val, x_amazon_f10_ternary_test, y_amazon_f10_ternary_train, y_amazon_f10_ternary_val, y_amazon_f10_ternary_test = torch.FloatTensor(x_amazon_f10_ternary_train), torch.FloatTensor(x_amazon_f10_ternary_val), torch.FloatTensor(x_amazon_f10_ternary_test), torch.LongTensor(y_amazon_f10_ternary_train), torch.LongTensor(y_amazon_f10_ternary_val), torch.LongTensor(y_amazon_f10_ternary_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77495276",
   "metadata": {},
   "source": [
    "Initializing the data loaders for different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14cdc6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "google_f10_train_dataset = TensorDataset(x_google_f10_train, y_google_f10_train)\n",
    "google_f10_train_dataset = data(google_f10_train_dataset)\n",
    "google_f10_val_dataset = TensorDataset(x_google_f10_val, y_google_f10_val)\n",
    "google_f10_val_dataset = data(google_f10_val_dataset)\n",
    "google_f10_test_dataset = TensorDataset(x_google_f10_test, y_google_f10_test)\n",
    "google_f10_test_dataset = data(google_f10_test_dataset)\n",
    "\n",
    "google_f10_train_loader = DataLoader(google_f10_train_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "google_f10_val_loader = DataLoader(google_f10_val_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "google_f10_test_loader = DataLoader(google_f10_test_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "\n",
    "google_f10_ternary_train_dataset = TensorDataset(x_google_f10_ternary_train, y_google_f10_ternary_train)\n",
    "google_f10_ternary_train_dataset = data(google_f10_ternary_train_dataset)\n",
    "google_f10_ternary_val_dataset = TensorDataset(x_google_f10_ternary_val, y_google_f10_ternary_val)\n",
    "google_f10_ternary_val_dataset = data(google_f10_ternary_val_dataset)\n",
    "google_f10_ternary_test_dataset = TensorDataset(x_google_f10_ternary_test, y_google_f10_ternary_test)\n",
    "google_f10_ternary_test_dataset = data(google_f10_ternary_test_dataset)\n",
    "\n",
    "google_f10_ternary_train_loader = DataLoader(google_f10_ternary_train_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "google_f10_ternary_val_loader = DataLoader(google_f10_ternary_val_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "google_f10_ternary_test_loader = DataLoader(google_f10_ternary_test_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "\n",
    "amazon_f10_train_dataset = TensorDataset(x_amazon_f10_train, y_amazon_f10_train)\n",
    "amazon_f10_train_dataset = data(amazon_f10_train_dataset)\n",
    "amazon_f10_val_dataset = TensorDataset(x_amazon_f10_val, y_amazon_f10_val)\n",
    "amazon_f10_val_dataset = data(amazon_f10_val_dataset)\n",
    "amazon_f10_test_dataset = TensorDataset(x_amazon_f10_test, y_amazon_f10_test)\n",
    "amazon_f10_test_dataset = data(amazon_f10_test_dataset)\n",
    "\n",
    "amazon_f10_train_loader = DataLoader(amazon_f10_train_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "amazon_f10_val_loader = DataLoader(amazon_f10_val_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "amazon_f10_test_loader = DataLoader(amazon_f10_test_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "\n",
    "amazon_f10_ternary_train_dataset = TensorDataset(x_amazon_f10_ternary_train, y_amazon_f10_ternary_train)\n",
    "amazon_f10_ternary_train_dataset = data(amazon_f10_ternary_train_dataset)\n",
    "amazon_f10_ternary_val_dataset = TensorDataset(x_amazon_f10_ternary_val, y_amazon_f10_ternary_val)\n",
    "amazon_f10_ternary_val_dataset = data(amazon_f10_ternary_val_dataset)\n",
    "amazon_f10_ternary_test_dataset = TensorDataset(x_amazon_f10_ternary_test, y_amazon_f10_ternary_test)\n",
    "amazon_f10_ternary_test_dataset = data(amazon_f10_ternary_test_dataset)\n",
    "\n",
    "amazon_f10_ternary_train_loader = DataLoader(amazon_f10_ternary_train_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "amazon_f10_ternary_val_loader = DataLoader(amazon_f10_ternary_val_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "amazon_f10_ternary_test_loader = DataLoader(amazon_f10_ternary_test_dataset, batch_size = batch_size, drop_last = True, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7762a845",
   "metadata": {},
   "source": [
    "Declaring the class for the binary classification network. Hyperparameters: learning rate - 0.03, dropout - 0.25 and SGD optimizer for both google and amazon word2vec feature models. Keeping the epochs at 10 as the models tend to overfit the data after 10 epochs and thus no improvement in validation loss is seen after that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b81d7b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff_nn_binary_f10(\n",
      "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ff_nn_binary_f10(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ff_nn_binary_f10, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        self.fc1 = torch.nn.Linear(3000, hidden_1)\n",
    "        self.fc2 = torch.nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = torch.nn.Linear(hidden_2, 1)\n",
    "        self.dropout = torch.nn.Dropout(0.25)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = func.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = func.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = ff_nn_binary_f10().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d51abef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del optimizer\n",
    "del loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2e607389",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ff_nn_binary_f10().to(device)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8e294e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.027619 \tValidation Loss: 0.024006\n",
      "Validation loss decreased (inf --> 0.024006). Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.024168 \tValidation Loss: 0.023455\n",
      "Validation loss decreased (0.024006 --> 0.023455). Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.023029 \tValidation Loss: 0.023116\n",
      "Validation loss decreased (0.023455 --> 0.023116). Saving model...\n",
      "Epoch: 4 \tTraining Loss: 0.021966 \tValidation Loss: 0.023071\n",
      "Validation loss decreased (0.023116 --> 0.023071). Saving model...\n",
      "Epoch: 5 \tTraining Loss: 0.020911 \tValidation Loss: 0.023256\n",
      "Epoch: 6 \tTraining Loss: 0.019832 \tValidation Loss: 0.023382\n",
      "Epoch: 7 \tTraining Loss: 0.018757 \tValidation Loss: 0.023968\n",
      "Epoch: 8 \tTraining Loss: 0.017801 \tValidation Loss: 0.024939\n",
      "Epoch: 9 \tTraining Loss: 0.016821 \tValidation Loss: 0.025995\n",
      "Epoch: 10 \tTraining Loss: 0.016007 \tValidation Loss: 0.026265\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "valid_min_loss = np.inf\n",
    "train_loader = google_f10_train_loader\n",
    "val_loader = google_f10_val_loader\n",
    "test_loader = google_f10_test_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1), target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    for inputs, target in val_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1), target.float())\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(val_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss,valid_loss))\n",
    "    if valid_loss <= valid_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_min_loss, valid_loss))\n",
    "        torch.save(model.state_dict(), 'model/google_f10_model.pt')\n",
    "        valid_min_loss = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6865e568",
   "metadata": {},
   "source": [
    "After viewing the results below, it must be surprising to see that the average vector model managed to outperform the first 10 words vector model. Logically, the latter should be able to capture the underlying relation between the word vectors and the labels better. But that is not the case, why? The answer is pretty straight-forward. There are 3000 input features and just ~60k samples per label. This proportion (no.of datapoints / no.of features) is clearly very small and thus reduces the performance. One solution would be to increase the no.of datapoints. Another would be to use a much more complex model that could fit with just the given no.of datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "deeb9125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of Google Word2Vec features of first 10 words for binary classification model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     19994\n",
      "           1       0.78      0.78      0.78     19946\n",
      "\n",
      "    accuracy                           0.78     39940\n",
      "   macro avg       0.78      0.78      0.78     39940\n",
      "weighted avg       0.78      0.78      0.78     39940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "test_loader = google_f10_test_loader\n",
    "model = ff_nn_binary_f10().to(device)\n",
    "model.load_state_dict(torch.load('model/google_f10_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        y_test_pred = model(inputs)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_targ_list.append(target.cpu().numpy())\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "print(\"Statistics of Google Word2Vec features of first 10 words for binary classification model\")\n",
    "print(classification_report(y_targ_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "be1e42e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del optimizer\n",
    "del loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "12fc5f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ff_nn_binary_f10().to(device)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "de1a0e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.025965 \tValidation Loss: 0.023710\n",
      "Validation loss decreased (inf --> 0.023710). Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.023627 \tValidation Loss: 0.023347\n",
      "Validation loss decreased (0.023710 --> 0.023347). Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.022361 \tValidation Loss: 0.023157\n",
      "Validation loss decreased (0.023347 --> 0.023157). Saving model...\n",
      "Epoch: 4 \tTraining Loss: 0.021295 \tValidation Loss: 0.023167\n",
      "Epoch: 5 \tTraining Loss: 0.020079 \tValidation Loss: 0.023377\n",
      "Epoch: 6 \tTraining Loss: 0.019068 \tValidation Loss: 0.023652\n",
      "Epoch: 7 \tTraining Loss: 0.018117 \tValidation Loss: 0.024583\n",
      "Epoch: 8 \tTraining Loss: 0.017081 \tValidation Loss: 0.025771\n",
      "Epoch: 9 \tTraining Loss: 0.016279 \tValidation Loss: 0.025938\n",
      "Epoch: 10 \tTraining Loss: 0.015382 \tValidation Loss: 0.026504\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "valid_min_loss = np.inf\n",
    "train_loader = amazon_f10_train_loader\n",
    "val_loader = amazon_f10_val_loader\n",
    "test_loader = amazon_f10_test_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1), target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    for inputs, target in val_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1), target.float())\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(val_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss,valid_loss))\n",
    "    if valid_loss <= valid_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_min_loss, valid_loss))\n",
    "        torch.save(model.state_dict(), 'model/amazon_f10_model.pt')\n",
    "        valid_min_loss = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921a1da",
   "metadata": {},
   "source": [
    "Still, the amazon word2vec feature model outperforms the google model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "42e02acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of Amazon Word2Vec features of first 10 words for binary classification model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.81      0.78     19994\n",
      "           1       0.79      0.74      0.77     19946\n",
      "\n",
      "    accuracy                           0.78     39940\n",
      "   macro avg       0.78      0.78      0.78     39940\n",
      "weighted avg       0.78      0.78      0.78     39940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "test_loader = amazon_f10_test_loader\n",
    "model = ff_nn_binary_f10().to(device)\n",
    "model.load_state_dict(torch.load('model/amazon_f10_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        y_test_pred = model(inputs)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_targ_list.append(target.cpu().numpy())\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "print(\"Statistics of Amazon Word2Vec features of first 10 words for binary classification model\")\n",
    "print(classification_report(y_targ_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b280e",
   "metadata": {},
   "source": [
    "Declaring the class for the ternary classification network model. Hyperparameters: learning rate - 0.001, dropout - 0.2 and optimizer - Adam. The hyperparameters are same for both the google and amazon word2vec feature models. Epochs set to be 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "71654c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff_nn_ternary_f10(\n",
      "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ff_nn_ternary_f10(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ff_nn_ternary_f10, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        self.fc1 = torch.nn.Linear(3000, hidden_1)\n",
    "        self.fc2 = torch.nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = torch.nn.Linear(hidden_2, 3)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = func.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = func.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = ff_nn_ternary_f10()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1e7620bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights of ternary classification for Google word2vec feature model:\n",
      "[0.83287476 0.83376736 1.66676678]\n",
      "\n",
      "Class weights of ternary classification for Amazon word2vec feature model:\n",
      "[0.83287476 0.83376736 1.66676678]\n"
     ]
    }
   ],
   "source": [
    "np_ary = y_google_f10_ternary\n",
    "class_weights_google_f10 = class_weight.compute_class_weight('balanced', np.unique(np_ary), np_ary)\n",
    "print(\"Class weights of ternary classification for Google word2vec feature model:\")\n",
    "print(class_weights_google_f10)\n",
    "np_ary = y_amazon_f10_ternary\n",
    "class_weights_amazon_f10 = class_weight.compute_class_weight('balanced', np.unique(np_ary), np_ary)\n",
    "print(\"\\nClass weights of ternary classification for Amazon word2vec feature model:\")\n",
    "print(class_weights_amazon_f10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b30c74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del optimizer\n",
    "del loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "04d3c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ff_nn_ternary_f10().to(device)\n",
    "class_weights = torch.FloatTensor(class_weights_google_f10)\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9b74628b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.044378 \tValidation Loss: 0.042580\n",
      "Validation loss decreased (inf --> 0.042580). Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.042055 \tValidation Loss: 0.042112\n",
      "Validation loss decreased (0.042580 --> 0.042112). Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.040667 \tValidation Loss: 0.041951\n",
      "Validation loss decreased (0.042112 --> 0.041951). Saving model...\n",
      "Epoch: 4 \tTraining Loss: 0.039281 \tValidation Loss: 0.042345\n",
      "Epoch: 5 \tTraining Loss: 0.038057 \tValidation Loss: 0.043076\n",
      "Epoch: 6 \tTraining Loss: 0.036943 \tValidation Loss: 0.043428\n",
      "Epoch: 7 \tTraining Loss: 0.035902 \tValidation Loss: 0.043986\n",
      "Epoch: 8 \tTraining Loss: 0.034921 \tValidation Loss: 0.044366\n",
      "Epoch: 9 \tTraining Loss: 0.034043 \tValidation Loss: 0.045273\n",
      "Epoch: 10 \tTraining Loss: 0.033275 \tValidation Loss: 0.046074\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "valid_min_loss = np.inf\n",
    "train_loader = google_f10_ternary_train_loader\n",
    "val_loader = google_f10_ternary_val_loader\n",
    "test_loader = google_f10_ternary_test_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    for inputs, target in val_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, target)\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(val_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss, valid_loss))\n",
    "    if valid_loss <= valid_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_min_loss, valid_loss))\n",
    "        torch.save(model.state_dict(), 'model/google_f10_ternary_model.pt')\n",
    "        valid_min_loss = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e7c35",
   "metadata": {},
   "source": [
    "As we saw in the average vector FFNN ternary classification models, the performance of this ternary classification model is also pretty mediocre. And the reasons for it remain the same as the ones given for the average vetor FFNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ab014073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of Google Word2Vec features of fist 10 words for ternary classification model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68     19855\n",
      "           1       0.65      0.76      0.70     20109\n",
      "           2       0.47      0.10      0.16      9976\n",
      "\n",
      "    accuracy                           0.63     49940\n",
      "   macro avg       0.58      0.54      0.51     49940\n",
      "weighted avg       0.60      0.63      0.58     49940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "test_loader = google_f10_ternary_test_loader\n",
    "model = ff_nn_ternary_f10().to(device)\n",
    "model.load_state_dict(torch.load('model/google_f10_ternary_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        y_test_pred = model(inputs)\n",
    "        _, y_test_pred = torch.max(y_test_pred, 1)\n",
    "        y_pred_tag = y_test_pred\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_targ_list.append(target.cpu().numpy())\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "print(\"Statistics of Google Word2Vec features of fist 10 words for ternary classification model\")\n",
    "print(classification_report(y_targ_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "39d1279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del optimizer\n",
    "del loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8dc695f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ff_nn_ternary_f10().to(device)\n",
    "class_weights = torch.FloatTensor(class_weights_google_f10)\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3db54781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.044314 \tValidation Loss: 0.042600\n",
      "Validation loss decreased (inf --> 0.042600). Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.042040 \tValidation Loss: 0.042173\n",
      "Validation loss decreased (0.042600 --> 0.042173). Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.040572 \tValidation Loss: 0.042054\n",
      "Validation loss decreased (0.042173 --> 0.042054). Saving model...\n",
      "Epoch: 4 \tTraining Loss: 0.039175 \tValidation Loss: 0.042477\n",
      "Epoch: 5 \tTraining Loss: 0.037797 \tValidation Loss: 0.042906\n",
      "Epoch: 6 \tTraining Loss: 0.036613 \tValidation Loss: 0.043617\n",
      "Epoch: 7 \tTraining Loss: 0.035505 \tValidation Loss: 0.044600\n",
      "Epoch: 8 \tTraining Loss: 0.034552 \tValidation Loss: 0.045405\n",
      "Epoch: 9 \tTraining Loss: 0.033705 \tValidation Loss: 0.046492\n",
      "Epoch: 10 \tTraining Loss: 0.032878 \tValidation Loss: 0.047008\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "valid_min_loss = np.inf\n",
    "train_loader = amazon_f10_ternary_train_loader\n",
    "val_loader = amazon_f10_ternary_val_loader\n",
    "test_loader = amazon_f10_ternary_test_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    for inputs, target in val_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, target)\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(val_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss, valid_loss))\n",
    "    if valid_loss <= valid_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_min_loss, valid_loss))\n",
    "        torch.save(model.state_dict(), 'model/amazon_f10_ternary_model.pt')\n",
    "        valid_min_loss = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a7c922",
   "metadata": {},
   "source": [
    "In this case, the performances of the amazon and the google word2vec feature models are nearly the same. The only patent difference is seen in the F1 score of the neutral class. This shows that the amazon model manages to edge out the google model by a very thin margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4369f3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of Amazon Word2Vec features of fist 10 words for ternary classification model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.77      0.68     19854\n",
      "           1       0.66      0.73      0.69     20109\n",
      "           2       0.45      0.12      0.19      9977\n",
      "\n",
      "    accuracy                           0.63     49940\n",
      "   macro avg       0.57      0.54      0.52     49940\n",
      "weighted avg       0.60      0.63      0.59     49940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "test_loader = amazon_f10_ternary_test_loader\n",
    "model = ff_nn_ternary_f10().to(device)\n",
    "model.load_state_dict(torch.load('model/amazon_f10_ternary_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        y_test_pred = model(inputs)\n",
    "        _, y_test_pred = torch.max(y_test_pred, 1)\n",
    "        y_pred_tag = y_test_pred\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_targ_list.append(target.cpu().numpy())\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "print(\"Statistics of Amazon Word2Vec features of fist 10 words for ternary classification model\")\n",
    "print(classification_report(y_targ_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f7e6d",
   "metadata": {},
   "source": [
    "Conclusion: The neural network models manage to perform better than the simple models for the binary classification task. The ternary classification task performance is average for both the amazon and the google word2vec feature models indicating that either the dataset is not proper or that the model isn't complex enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0e3a79",
   "metadata": {},
   "source": [
    "## RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba48147",
   "metadata": {},
   "source": [
    "Declaring the function that calculates the word2vec features for the first 50 words of a review and stores them in separate dimensions, indicating the 50 timesteps. Once again, we throw away reviews with no words or reviews where none of the words are modeled by either of the word2vec models. If the review length is less than 50 words, we pad the remaining spaces with zero vectors, and this is taken care by initializing the result array as a numpy zeros array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894dbcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rnn_ip(sentence, model):\n",
    "    split_list = sentence.split(\" \")\n",
    "    result_array = np.zeros((50, 300), dtype = np.float32)\n",
    "    idx = 0\n",
    "    if model == \"google\":\n",
    "        for word in split_list:\n",
    "            if idx == 50 or idx == len(split_list):\n",
    "                break\n",
    "            try:\n",
    "                result_array[idx] = w2v_google[word]\n",
    "                idx += 1\n",
    "            except KeyError:\n",
    "                continue\n",
    "    else:\n",
    "        for word in split_list:\n",
    "            if idx == 50 or idx == len(split_list):\n",
    "                break\n",
    "            try:\n",
    "                result_array[idx] = w2v_model.wv[word]\n",
    "                idx += 1\n",
    "            except KeyError:\n",
    "                continue\n",
    "    if idx == 0:\n",
    "        return np.nan\n",
    "    return result_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5eb03b",
   "metadata": {},
   "source": [
    "As we did before, we create a list of tuples depicting the dataset and we drop the datapoints with nan values. Then we store the datasets as pickle files for easier and faster access in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26186ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_google_df = [(compute_rnn_ip(df_new.iloc[[x]].review_body.values[0], \"google\"), df_new.iloc[[x]].star_rating.values[0]) for x in range(len(df_new))]\n",
    "rnn_google_df = [x for x in rnn_google_df if np.isnan(x[0]).any() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42fc14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/rnn_google.pkl\", \"wb\") as file_obj:\n",
    "    pkl.dump(rnn_google_df, file_obj)\n",
    "\n",
    "del rnn_google_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329bfc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_amazon_df = [(compute_rnn_ip(df_new.iloc[[x]].review_body.values[0], \"amazon\"), df_new.iloc[[x]].star_rating.values[0]) for x in range(len(df_new))]\n",
    "rnn_amazon_df = [x for x in rnn_amazon_df if np.isnan(x[0]).any() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd0335",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/rnn_amazon.pkl\", \"wb\") as file_obj:\n",
    "    pkl.dump(rnn_amazon_df, file_obj)\n",
    "\n",
    "del rnn_amazon_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f3ba9a",
   "metadata": {},
   "source": [
    "##### Checkpoint - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afa4da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/rnn_google.pkl\", \"rb\") as file_obj:\n",
    "    rnn_google_df = pkl.load(file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "388e1ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/rnn_amazon.pkl\", \"rb\") as file_obj:\n",
    "    rnn_amazon_df = pkl.load(file_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2f855e",
   "metadata": {},
   "source": [
    "Now, we form the binary dataset and then separate both the binary and ternary datasets into inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "796334ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_google_bin = [x for x in rnn_google_df if x[1] == 0 or x[1] == 1]\n",
    "rnn_amazon_bin = [x for x in rnn_amazon_df if x[1] == 0 or x[1] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba8eb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_google_x = np.array([x[0] for x in rnn_google_bin])\n",
    "rnn_google_y = np.array([x[1] for x in rnn_google_bin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95cd5345",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_amazon_x = np.array([x[0] for x in rnn_amazon_bin])\n",
    "rnn_amazon_y = np.array([x[1] for x in rnn_amazon_bin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f2505f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_google_ternary_x = np.array([x[0] for x in rnn_google_df])\n",
    "rnn_google_ternary_y = np.array([x[1] for x in rnn_google_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4996473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_amazon_ternary_x = np.array([x[0] for x in rnn_amazon_df])\n",
    "rnn_amazon_ternary_y = np.array([x[1] for x in rnn_amazon_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3f5f92",
   "metadata": {},
   "source": [
    "Initial 80/20 split for train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48a2a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_google_x_train, rnn_google_x_test, rnn_google_y_train, rnn_google_y_test = train_test_split(rnn_google_x, rnn_google_y, test_size=0.2, random_state=42)\n",
    "rnn_amazon_x_train, rnn_amazon_x_test, rnn_amazon_y_train, rnn_amazon_y_test = train_test_split(rnn_amazon_x, rnn_amazon_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a97b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_google_x_ternary_train, rnn_google_x_ternary_test, rnn_google_y_ternary_train, rnn_google_y_ternary_test = train_test_split(rnn_google_ternary_x, rnn_google_ternary_y, test_size=0.2, random_state=42)\n",
    "rnn_amazon_x_ternary_train, rnn_amazon_x_ternary_test, rnn_amazon_y_ternary_train, rnn_amazon_y_ternary_test = train_test_split(rnn_amazon_ternary_x, rnn_amazon_ternary_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2881b3b8",
   "metadata": {},
   "source": [
    "80/20/20 split for train, val and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21318864",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_google_x_train, rnn_google_x_val, rnn_google_y_train, rnn_google_y_val = train_test_split(rnn_google_x_train, rnn_google_y_train, test_size=0.2, random_state=42)\n",
    "rnn_amazon_x_train, rnn_amazon_x_val, rnn_amazon_y_train, rnn_amazon_y_val = train_test_split(rnn_amazon_x_train, rnn_amazon_y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "rnn_google_x_ternary_train, rnn_google_x_ternary_val, rnn_google_y_ternary_train, rnn_google_y_ternary_val = train_test_split(rnn_google_x_ternary_train, rnn_google_y_ternary_train, test_size=0.2, random_state=42)\n",
    "rnn_amazon_x_ternary_train, rnn_amazon_x_ternary_val, rnn_amazon_y_ternary_train, rnn_amazon_y_ternary_val = train_test_split(rnn_amazon_x_ternary_train, rnn_amazon_y_ternary_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dbbab9",
   "metadata": {},
   "source": [
    "Converting the numpy ndarrays to float and long tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05ad1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_google_x_train, rnn_google_x_val, rnn_google_x_test, rnn_google_y_train, rnn_google_y_val, rnn_google_y_test = torch.FloatTensor(rnn_google_x_train), torch.FloatTensor(rnn_google_x_val), torch.FloatTensor(rnn_google_x_test), torch.LongTensor(rnn_google_y_train), torch.LongTensor(rnn_google_y_val), torch.LongTensor(rnn_google_y_test)\n",
    "rnn_amazon_x_train, rnn_amazon_x_val, rnn_amazon_x_test, rnn_amazon_y_train, rnn_amazon_y_val, rnn_amazon_y_test = torch.FloatTensor(rnn_amazon_x_train), torch.FloatTensor(rnn_amazon_x_val), torch.FloatTensor(rnn_amazon_x_test), torch.LongTensor(rnn_amazon_y_train), torch.LongTensor(rnn_amazon_y_val), torch.LongTensor(rnn_amazon_y_test)\n",
    "\n",
    "rnn_google_x_ternary_train, rnn_google_x_ternary_val, rnn_google_x_ternary_test, rnn_google_y_ternary_train, rnn_google_y_ternary_val, rnn_google_y_ternary_test = torch.FloatTensor(rnn_google_x_ternary_train), torch.FloatTensor(rnn_google_x_ternary_val), torch.FloatTensor(rnn_google_x_ternary_test), torch.LongTensor(rnn_google_y_ternary_train), torch.LongTensor(rnn_google_y_ternary_val), torch.LongTensor(rnn_google_y_ternary_test)\n",
    "rnn_amazon_x_ternary_train, rnn_amazon_x_ternary_val, rnn_amazon_x_ternary_test, rnn_amazon_y_ternary_train, rnn_amazon_y_ternary_val, rnn_amazon_y_ternary_test = torch.FloatTensor(rnn_amazon_x_ternary_train), torch.FloatTensor(rnn_amazon_x_ternary_val), torch.FloatTensor(rnn_amazon_x_ternary_test), torch.LongTensor(rnn_amazon_y_ternary_train), torch.LongTensor(rnn_amazon_y_ternary_val), torch.LongTensor(rnn_amazon_y_ternary_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99610e1",
   "metadata": {},
   "source": [
    "Deleting some variables to free-up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e55391a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del rnn_google_bin, rnn_google_df, rnn_google_x, rnn_google_y, rnn_google_ternary_x, rnn_google_ternary_y\n",
    "del rnn_amazon_bin, rnn_amazon_df, rnn_amazon_x, rnn_amazon_y, rnn_amazon_ternary_x, rnn_amazon_ternary_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea79020b",
   "metadata": {},
   "source": [
    "Initializing the dataloaders for all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f263858",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "rnn_google_train_dataset = TensorDataset(rnn_google_x_train, rnn_google_y_train)\n",
    "rnn_google_train_dataset = data(rnn_google_train_dataset)\n",
    "rnn_google_val_dataset = TensorDataset(rnn_google_x_val, rnn_google_y_val)\n",
    "rnn_google_val_dataset = data(rnn_google_val_dataset)\n",
    "rnn_google_test_dataset = TensorDataset(rnn_google_x_test, rnn_google_y_test)\n",
    "rnn_google_test_dataset = data(rnn_google_test_dataset)\n",
    "\n",
    "rnn_google_train_loader = DataLoader(rnn_google_train_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "rnn_google_val_loader = DataLoader(rnn_google_val_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "rnn_google_test_loader = DataLoader(rnn_google_test_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "\n",
    "rnn_amazon_train_dataset = TensorDataset(rnn_amazon_x_train, rnn_amazon_y_train)\n",
    "rnn_amazon_train_dataset = data(rnn_amazon_train_dataset)\n",
    "rnn_amazon_val_dataset = TensorDataset(rnn_amazon_x_val, rnn_amazon_y_val)\n",
    "rnn_amazon_val_dataset = data(rnn_amazon_val_dataset)\n",
    "rnn_amazon_test_dataset = TensorDataset(rnn_amazon_x_test, rnn_amazon_y_test)\n",
    "rnn_amazon_test_dataset = data(rnn_amazon_test_dataset)\n",
    "\n",
    "rnn_amazon_train_loader = DataLoader(rnn_amazon_train_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "rnn_amazon_val_loader = DataLoader(rnn_amazon_val_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "rnn_amazon_test_loader = DataLoader(rnn_amazon_test_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "\n",
    "rnn_google_ternary_train_dataset = TensorDataset(rnn_google_x_ternary_train, rnn_google_y_ternary_train)\n",
    "rnn_google_ternary_train_dataset = data(rnn_google_ternary_train_dataset)\n",
    "rnn_google_ternary_val_dataset = TensorDataset(rnn_google_x_ternary_val, rnn_google_y_ternary_val)\n",
    "rnn_google_ternary_val_dataset = data(rnn_google_ternary_val_dataset)\n",
    "rnn_google_ternary_test_dataset = TensorDataset(rnn_google_x_ternary_test, rnn_google_y_ternary_test)\n",
    "rnn_google_ternary_test_dataset = data(rnn_google_ternary_test_dataset)\n",
    "\n",
    "rnn_google_ternary_train_loader = DataLoader(rnn_google_ternary_train_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "rnn_google_ternary_val_loader = DataLoader(rnn_google_ternary_val_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "rnn_google_ternary_test_loader = DataLoader(rnn_google_ternary_test_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "\n",
    "rnn_amazon_ternary_train_dataset = TensorDataset(rnn_amazon_x_ternary_train, rnn_amazon_y_ternary_train)\n",
    "rnn_amazon_ternary_train_dataset = data(rnn_amazon_ternary_train_dataset)\n",
    "rnn_amazon_ternary_val_dataset = TensorDataset(rnn_amazon_x_ternary_val, rnn_amazon_y_ternary_val)\n",
    "rnn_amazon_ternary_val_dataset = data(rnn_amazon_ternary_val_dataset)\n",
    "rnn_amazon_ternary_test_dataset = TensorDataset(rnn_amazon_x_ternary_test, rnn_amazon_y_ternary_test)\n",
    "rnn_amazon_ternary_test_dataset = data(rnn_amazon_ternary_test_dataset)\n",
    "\n",
    "rnn_amazon_ternary_train_loader = DataLoader(rnn_amazon_ternary_train_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "rnn_amazon_ternary_val_loader = DataLoader(rnn_amazon_ternary_val_dataset, batch_size = batch_size, drop_last = True, shuffle = True)\n",
    "rnn_amazon_ternary_test_loader = DataLoader(rnn_amazon_ternary_test_dataset, batch_size = batch_size, drop_last = True, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83a6226",
   "metadata": {},
   "source": [
    "Declaring the class for the RNN binary classification network. Hyperparameters: learning rate - 0.0001, epochs - 20 and Adam optimizer is used across both the amazon and the google word2vec feature models for both, binary and ternary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf96e843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(300, 50, batch_first=True)\n",
      "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = 50\n",
    "        self.n_layers = 1\n",
    "        self.rnn = torch.nn.RNN(300, self.hidden_size, self.n_layers, batch_first = True)\n",
    "        self.fc = torch.nn.Linear(self.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        return hidden\n",
    "\n",
    "model = RNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3df3da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN()\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93693df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.028130 \tValidation Loss: 0.023682\n",
      "Validation loss decreased (inf --> 0.023682). Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.022557 \tValidation Loss: 0.021149\n",
      "Validation loss decreased (0.023682 --> 0.021149). Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.021351 \tValidation Loss: 0.020905\n",
      "Validation loss decreased (0.021149 --> 0.020905). Saving model...\n",
      "Epoch: 4 \tTraining Loss: 0.021047 \tValidation Loss: 0.020600\n",
      "Validation loss decreased (0.020905 --> 0.020600). Saving model...\n",
      "Epoch: 5 \tTraining Loss: 0.020800 \tValidation Loss: 0.020518\n",
      "Validation loss decreased (0.020600 --> 0.020518). Saving model...\n",
      "Epoch: 6 \tTraining Loss: 0.020527 \tValidation Loss: 0.020908\n",
      "Epoch: 7 \tTraining Loss: 0.020330 \tValidation Loss: 0.020780\n",
      "Epoch: 8 \tTraining Loss: 0.020230 \tValidation Loss: 0.020546\n",
      "Epoch: 9 \tTraining Loss: 0.020022 \tValidation Loss: 0.019833\n",
      "Validation loss decreased (0.020518 --> 0.019833). Saving model...\n",
      "Epoch: 10 \tTraining Loss: 0.019861 \tValidation Loss: 0.019696\n",
      "Validation loss decreased (0.019833 --> 0.019696). Saving model...\n",
      "Epoch: 11 \tTraining Loss: 0.019636 \tValidation Loss: 0.019717\n",
      "Epoch: 12 \tTraining Loss: 0.019419 \tValidation Loss: 0.019381\n",
      "Validation loss decreased (0.019696 --> 0.019381). Saving model...\n",
      "Epoch: 13 \tTraining Loss: 0.019168 \tValidation Loss: 0.019230\n",
      "Validation loss decreased (0.019381 --> 0.019230). Saving model...\n",
      "Epoch: 14 \tTraining Loss: 0.018866 \tValidation Loss: 0.018672\n",
      "Validation loss decreased (0.019230 --> 0.018672). Saving model...\n",
      "Epoch: 15 \tTraining Loss: 0.018650 \tValidation Loss: 0.018988\n",
      "Epoch: 16 \tTraining Loss: 0.018409 \tValidation Loss: 0.018406\n",
      "Validation loss decreased (0.018672 --> 0.018406). Saving model...\n",
      "Epoch: 17 \tTraining Loss: 0.018122 \tValidation Loss: 0.019149\n",
      "Epoch: 18 \tTraining Loss: 0.017955 \tValidation Loss: 0.018150\n",
      "Validation loss decreased (0.018406 --> 0.018150). Saving model...\n",
      "Epoch: 19 \tTraining Loss: 0.017764 \tValidation Loss: 0.018334\n",
      "Epoch: 20 \tTraining Loss: 0.017607 \tValidation Loss: 0.017860\n",
      "Validation loss decreased (0.018150 --> 0.017860). Saving model...\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "valid_min_loss = np.inf\n",
    "train_loader = rnn_google_train_loader\n",
    "val_loader = rnn_google_val_loader\n",
    "test_loader = rnn_google_test_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1), target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    for inputs, target in val_loader:\n",
    "        inputs, target = inputs, target\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1), target.float())\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(val_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss,valid_loss))\n",
    "    if valid_loss <= valid_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_min_loss, valid_loss))\n",
    "        torch.save(model.state_dict(), 'model/rnn_google_model.pt')\n",
    "        valid_min_loss = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f3d4b4",
   "metadata": {},
   "source": [
    "Clearly, the performance of the RNN model is better than all of the previous models for the google word2vec feature model for the binary classification task. This proves that, if not already apparent, that the RNN is much more complex than all the previous models. Another point to note is that the validation loss seems to be decreasing even at the 20th epoch which indicates that the model could be further improved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d24ec749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of Google Word2Vec features of first 50 words for binary classification RNN model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85     20025\n",
      "           1       0.86      0.82      0.84     19935\n",
      "\n",
      "    accuracy                           0.84     39960\n",
      "   macro avg       0.85      0.84      0.84     39960\n",
      "weighted avg       0.85      0.84      0.84     39960\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "test_loader = rnn_google_test_loader\n",
    "model = RNN()\n",
    "model.load_state_dict(torch.load('model/rnn_google_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        y_test_pred = model(inputs)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.numpy())\n",
    "        y_targ_list.append(target.numpy())\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "print(\"Statistics of Google Word2Vec features of first 50 words for binary classification RNN model\")\n",
    "print(classification_report(y_targ_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20ba3c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del optimizer\n",
    "del loss_fn\n",
    "model = RNN()\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecf57bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.033635 \tValidation Loss: 0.030491\n",
      "Validation loss decreased (inf --> 0.030491). Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.029039 \tValidation Loss: 0.028165\n",
      "Validation loss decreased (0.030491 --> 0.028165). Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.026950 \tValidation Loss: 0.026156\n",
      "Validation loss decreased (0.028165 --> 0.026156). Saving model...\n",
      "Epoch: 4 \tTraining Loss: 0.025527 \tValidation Loss: 0.025191\n",
      "Validation loss decreased (0.026156 --> 0.025191). Saving model...\n",
      "Epoch: 5 \tTraining Loss: 0.023178 \tValidation Loss: 0.021349\n",
      "Validation loss decreased (0.025191 --> 0.021349). Saving model...\n",
      "Epoch: 6 \tTraining Loss: 0.019584 \tValidation Loss: 0.019112\n",
      "Validation loss decreased (0.021349 --> 0.019112). Saving model...\n",
      "Epoch: 7 \tTraining Loss: 0.018305 \tValidation Loss: 0.018093\n",
      "Validation loss decreased (0.019112 --> 0.018093). Saving model...\n",
      "Epoch: 8 \tTraining Loss: 0.017701 \tValidation Loss: 0.018711\n",
      "Epoch: 9 \tTraining Loss: 0.017368 \tValidation Loss: 0.017402\n",
      "Validation loss decreased (0.018093 --> 0.017402). Saving model...\n",
      "Epoch: 10 \tTraining Loss: 0.017144 \tValidation Loss: 0.017777\n",
      "Epoch: 11 \tTraining Loss: 0.016865 \tValidation Loss: 0.017761\n",
      "Epoch: 12 \tTraining Loss: 0.016636 \tValidation Loss: 0.017736\n",
      "Epoch: 13 \tTraining Loss: 0.016472 \tValidation Loss: 0.017178\n",
      "Validation loss decreased (0.017402 --> 0.017178). Saving model...\n",
      "Epoch: 14 \tTraining Loss: 0.016397 \tValidation Loss: 0.017538\n",
      "Epoch: 15 \tTraining Loss: 0.016289 \tValidation Loss: 0.016869\n",
      "Validation loss decreased (0.017178 --> 0.016869). Saving model...\n",
      "Epoch: 16 \tTraining Loss: 0.016096 \tValidation Loss: 0.017599\n",
      "Epoch: 17 \tTraining Loss: 0.015973 \tValidation Loss: 0.016998\n",
      "Epoch: 18 \tTraining Loss: 0.016035 \tValidation Loss: 0.016797\n",
      "Validation loss decreased (0.016869 --> 0.016797). Saving model...\n",
      "Epoch: 19 \tTraining Loss: 0.015983 \tValidation Loss: 0.017190\n",
      "Epoch: 20 \tTraining Loss: 0.015783 \tValidation Loss: 0.017995\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "valid_min_loss = np.inf\n",
    "train_loader = rnn_amazon_train_loader\n",
    "val_loader = rnn_amazon_val_loader\n",
    "test_loader = rnn_amazon_test_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1), target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    for inputs, target in val_loader:\n",
    "        inputs, target = inputs, target\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1), target.float())\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(val_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss,valid_loss))\n",
    "    if valid_loss <= valid_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_min_loss, valid_loss))\n",
    "        torch.save(model.state_dict(), 'model/rnn_amazon_model.pt')\n",
    "        valid_min_loss = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ffa08",
   "metadata": {},
   "source": [
    "As noted with the previous models, the amazon word2vec feature model once again outperforms its counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78bec13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of Amazon Word2Vec features of first 50 words for binary classification RNN model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.85     19992\n",
      "           1       0.85      0.86      0.86     19948\n",
      "\n",
      "    accuracy                           0.86     39940\n",
      "   macro avg       0.86      0.86      0.86     39940\n",
      "weighted avg       0.86      0.86      0.86     39940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "test_loader = rnn_amazon_test_loader\n",
    "model = RNN()\n",
    "model.load_state_dict(torch.load('model/rnn_amazon_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        y_test_pred = model(inputs)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.numpy())\n",
    "        y_targ_list.append(target.numpy())\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "print(\"Statistics of Amazon Word2Vec features of first 50 words for binary classification RNN model\")\n",
    "print(classification_report(y_targ_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "32f309ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_ternary(\n",
      "  (rnn): RNN(300, 50, batch_first=True)\n",
      "  (fc): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RNN_ternary(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN_ternary, self).__init__()\n",
    "        self.hidden_size = 50\n",
    "        self.n_layers = 1\n",
    "        self.rnn = torch.nn.RNN(300, self.hidden_size, self.n_layers, batch_first = True)\n",
    "        self.fc = torch.nn.Linear(self.hidden_size, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        return hidden\n",
    "\n",
    "model = RNN_ternary()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968b4975",
   "metadata": {},
   "source": [
    "Once again, we are initializing class weights for proper performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a5efae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights of ternary classification for Google word2vec feature model:\n",
      "tensor([0.8297, 0.8367, 1.6678], dtype=torch.float64)\n",
      "\n",
      "Class weights of ternary classification for Amazon word2vec feature model:\n",
      "tensor([0.8303, 0.8372, 1.6635], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "np_ary = rnn_google_y_ternary_train.numpy()\n",
    "class_weights_rnn_google = torch.tensor(class_weight.compute_class_weight('balanced', np.unique(np_ary), np_ary))\n",
    "print(\"Class weights of ternary classification for Google word2vec feature model:\")\n",
    "print(class_weights_rnn_google)\n",
    "np_ary = rnn_amazon_y_ternary_train.numpy()\n",
    "class_weights_rnn_amazon = torch.tensor(class_weight.compute_class_weight('balanced', np.unique(np_ary), np_ary))\n",
    "print(\"\\nClass weights of ternary classification for Amazon word2vec feature model:\")\n",
    "print(class_weights_rnn_amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "31607bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del optimizer\n",
    "del loss_fn\n",
    "model = RNN_ternary()\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight = class_weights_rnn_google)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "756945cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.050586 \tValidation Loss: 0.046852\n",
      "Validation loss decreased (inf --> 0.046852). Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.046015 \tValidation Loss: 0.045228\n",
      "Validation loss decreased (0.046852 --> 0.045228). Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.045355 \tValidation Loss: 0.045180\n",
      "Validation loss decreased (0.045228 --> 0.045180). Saving model...\n",
      "Epoch: 4 \tTraining Loss: 0.045016 \tValidation Loss: 0.044481\n",
      "Validation loss decreased (0.045180 --> 0.044481). Saving model...\n",
      "Epoch: 5 \tTraining Loss: 0.044509 \tValidation Loss: 0.043520\n",
      "Validation loss decreased (0.044481 --> 0.043520). Saving model...\n",
      "Epoch: 6 \tTraining Loss: 0.043681 \tValidation Loss: 0.043697\n",
      "Epoch: 7 \tTraining Loss: 0.043380 \tValidation Loss: 0.043219\n",
      "Validation loss decreased (0.043520 --> 0.043219). Saving model...\n",
      "Epoch: 8 \tTraining Loss: 0.043116 \tValidation Loss: 0.042657\n",
      "Validation loss decreased (0.043219 --> 0.042657). Saving model...\n",
      "Epoch: 9 \tTraining Loss: 0.042881 \tValidation Loss: 0.043037\n",
      "Epoch: 10 \tTraining Loss: 0.042668 \tValidation Loss: 0.042728\n",
      "Epoch: 11 \tTraining Loss: 0.042460 \tValidation Loss: 0.041775\n",
      "Validation loss decreased (0.042657 --> 0.041775). Saving model...\n",
      "Epoch: 12 \tTraining Loss: 0.042222 \tValidation Loss: 0.042720\n",
      "Epoch: 13 \tTraining Loss: 0.042038 \tValidation Loss: 0.042552\n",
      "Epoch: 14 \tTraining Loss: 0.041722 \tValidation Loss: 0.041787\n",
      "Epoch: 15 \tTraining Loss: 0.041461 \tValidation Loss: 0.042603\n",
      "Epoch: 16 \tTraining Loss: 0.041257 \tValidation Loss: 0.040696\n",
      "Validation loss decreased (0.041775 --> 0.040696). Saving model...\n",
      "Epoch: 17 \tTraining Loss: 0.041033 \tValidation Loss: 0.040649\n",
      "Validation loss decreased (0.040696 --> 0.040649). Saving model...\n",
      "Epoch: 18 \tTraining Loss: 0.040832 \tValidation Loss: 0.040650\n",
      "Epoch: 19 \tTraining Loss: 0.040633 \tValidation Loss: 0.040331\n",
      "Validation loss decreased (0.040649 --> 0.040331). Saving model...\n",
      "Epoch: 20 \tTraining Loss: 0.040456 \tValidation Loss: 0.040361\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "valid_min_loss = np.inf\n",
    "train_loader = rnn_google_ternary_train_loader\n",
    "val_loader = rnn_google_ternary_val_loader\n",
    "test_loader = rnn_google_ternary_test_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1).double(), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    for inputs, target in val_loader:\n",
    "        inputs, target = inputs, target\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1).double(), target)\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(val_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss,valid_loss))\n",
    "    if valid_loss <= valid_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_min_loss, valid_loss))\n",
    "        torch.save(model.state_dict(), 'model/rnn_google_ternary_model.pt')\n",
    "        valid_min_loss = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b0ec42",
   "metadata": {},
   "source": [
    "From the below stats, it can be noted that there is a slight increase in the performance when compared to previous models for the ternary classification task. That being said, overall, the performance is still mediocre. The reasons for such performance continue to remain the same as mentioned before (improper dataset or not-so-complex model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5dd81db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of Google Word2Vec features of fist 50 words for ternary classification RNN model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.65      0.70     19791\n",
      "           1       0.81      0.68      0.74     20168\n",
      "           2       0.33      0.54      0.41      9981\n",
      "\n",
      "    accuracy                           0.64     49940\n",
      "   macro avg       0.64      0.62      0.62     49940\n",
      "weighted avg       0.70      0.64      0.66     49940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "test_loader = rnn_google_ternary_test_loader\n",
    "model = RNN_ternary()\n",
    "model.load_state_dict(torch.load('model/rnn_google_ternary_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        y_test_pred = model(inputs)\n",
    "        _, y_test_pred = torch.max(y_test_pred, 1)\n",
    "        y_pred_tag = y_test_pred\n",
    "        y_pred_list.append(y_pred_tag.numpy())\n",
    "        y_targ_list.append(target.numpy())\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "print(\"Statistics of Google Word2Vec features of fist 50 words for ternary classification RNN model\")\n",
    "print(classification_report(y_targ_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e818c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del optimizer\n",
    "del loss_fn\n",
    "model = RNN_ternary()\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight = class_weights_rnn_amazon)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a3bdb43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.052677 \tValidation Loss: 0.050518\n",
      "Validation loss decreased (inf --> 0.050518). Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.049854 \tValidation Loss: 0.048954\n",
      "Validation loss decreased (0.050518 --> 0.048954). Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.048552 \tValidation Loss: 0.048463\n",
      "Validation loss decreased (0.048954 --> 0.048463). Saving model...\n",
      "Epoch: 4 \tTraining Loss: 0.047926 \tValidation Loss: 0.047600\n",
      "Validation loss decreased (0.048463 --> 0.047600). Saving model...\n",
      "Epoch: 5 \tTraining Loss: 0.046806 \tValidation Loss: 0.046919\n",
      "Validation loss decreased (0.047600 --> 0.046919). Saving model...\n",
      "Epoch: 6 \tTraining Loss: 0.045091 \tValidation Loss: 0.044774\n",
      "Validation loss decreased (0.046919 --> 0.044774). Saving model...\n",
      "Epoch: 7 \tTraining Loss: 0.044653 \tValidation Loss: 0.045930\n",
      "Epoch: 8 \tTraining Loss: 0.045074 \tValidation Loss: 0.045071\n",
      "Epoch: 9 \tTraining Loss: 0.044252 \tValidation Loss: 0.045015\n",
      "Epoch: 10 \tTraining Loss: 0.044645 \tValidation Loss: 0.044268\n",
      "Validation loss decreased (0.044774 --> 0.044268). Saving model...\n",
      "Epoch: 11 \tTraining Loss: 0.044407 \tValidation Loss: 0.045361\n",
      "Epoch: 12 \tTraining Loss: 0.044883 \tValidation Loss: 0.044832\n",
      "Epoch: 13 \tTraining Loss: 0.044741 \tValidation Loss: 0.044827\n",
      "Epoch: 14 \tTraining Loss: 0.043997 \tValidation Loss: 0.046762\n",
      "Epoch: 15 \tTraining Loss: 0.044699 \tValidation Loss: 0.044635\n",
      "Epoch: 16 \tTraining Loss: 0.044234 \tValidation Loss: 0.044856\n",
      "Epoch: 17 \tTraining Loss: 0.044740 \tValidation Loss: 0.044362\n",
      "Epoch: 18 \tTraining Loss: 0.044903 \tValidation Loss: 0.045581\n",
      "Epoch: 19 \tTraining Loss: 0.045098 \tValidation Loss: 0.044720\n",
      "Epoch: 20 \tTraining Loss: 0.045394 \tValidation Loss: 0.045415\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "valid_min_loss = np.inf\n",
    "train_loader = rnn_amazon_ternary_train_loader\n",
    "val_loader = rnn_amazon_ternary_val_loader\n",
    "test_loader = rnn_amazon_ternary_test_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1).double(), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    for inputs, target in val_loader:\n",
    "        inputs, target = inputs, target\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1).double(), target)\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(val_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss,valid_loss))\n",
    "    if valid_loss <= valid_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_min_loss, valid_loss))\n",
    "        torch.save(model.state_dict(), 'model/rnn_amazon_ternary_model.pt')\n",
    "        valid_min_loss = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "206333db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of Amazon Word2Vec features of fist 50 words for ternary classification RNN model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.64      0.69     19865\n",
      "           1       0.74      0.68      0.71     20087\n",
      "           2       0.30      0.43      0.36      9968\n",
      "\n",
      "    accuracy                           0.62     49920\n",
      "   macro avg       0.59      0.59      0.58     49920\n",
      "weighted avg       0.65      0.62      0.63     49920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "test_loader = rnn_amazon_ternary_test_loader\n",
    "model = RNN_ternary()\n",
    "model.load_state_dict(torch.load('model/rnn_amazon_ternary_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        y_test_pred = model(inputs)\n",
    "        _, y_test_pred = torch.max(y_test_pred, 1)\n",
    "        y_pred_tag = y_test_pred\n",
    "        y_pred_list.append(y_pred_tag.numpy())\n",
    "        y_targ_list.append(target.numpy())\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "print(\"Statistics of Amazon Word2Vec features of fist 50 words for ternary classification RNN model\")\n",
    "print(classification_report(y_targ_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cfaf33",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd7671a",
   "metadata": {},
   "source": [
    "Declaring the class for the GRU binary classification network. The hyperparameters for the GRU models (both amazon and google word2vec feature models) for both tasks (binary and ternary classification) remain the same as the RNN models. Learning rate - 0.0001, epochs - 20 and Adam optimizer. We even use the same dataloaders as the ones that were used with the RNN models since the data for both networks are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "bea912f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU(\n",
      "  (rnn): GRU(300, 50, batch_first=True)\n",
      "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GRU(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = 50\n",
    "        self.n_layers = 1\n",
    "        self.rnn = torch.nn.GRU(300, self.hidden_size, self.n_layers, batch_first = True)\n",
    "        self.fc = torch.nn.Linear(self.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        return hidden\n",
    "\n",
    "model = GRU()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "75746df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del optimizer\n",
    "del loss_fn\n",
    "model = GRU()\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "57d35ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.024937 \tValidation Loss: 0.019186\n",
      "Validation loss decreased (inf --> 0.019186). Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.018449 \tValidation Loss: 0.017737\n",
      "Validation loss decreased (0.019186 --> 0.017737). Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.017251 \tValidation Loss: 0.016776\n",
      "Validation loss decreased (0.017737 --> 0.016776). Saving model...\n",
      "Epoch: 4 \tTraining Loss: 0.016481 \tValidation Loss: 0.016210\n",
      "Validation loss decreased (0.016776 --> 0.016210). Saving model...\n",
      "Epoch: 5 \tTraining Loss: 0.015913 \tValidation Loss: 0.015737\n",
      "Validation loss decreased (0.016210 --> 0.015737). Saving model...\n",
      "Epoch: 6 \tTraining Loss: 0.015534 \tValidation Loss: 0.015436\n",
      "Validation loss decreased (0.015737 --> 0.015436). Saving model...\n",
      "Epoch: 7 \tTraining Loss: 0.015141 \tValidation Loss: 0.015117\n",
      "Validation loss decreased (0.015436 --> 0.015117). Saving model...\n",
      "Epoch: 8 \tTraining Loss: 0.014885 \tValidation Loss: 0.014931\n",
      "Validation loss decreased (0.015117 --> 0.014931). Saving model...\n",
      "Epoch: 9 \tTraining Loss: 0.014654 \tValidation Loss: 0.014921\n",
      "Validation loss decreased (0.014931 --> 0.014921). Saving model...\n",
      "Epoch: 10 \tTraining Loss: 0.014406 \tValidation Loss: 0.014740\n",
      "Validation loss decreased (0.014921 --> 0.014740). Saving model...\n",
      "Epoch: 11 \tTraining Loss: 0.014209 \tValidation Loss: 0.014749\n",
      "Epoch: 12 \tTraining Loss: 0.014044 \tValidation Loss: 0.014598\n",
      "Validation loss decreased (0.014740 --> 0.014598). Saving model...\n",
      "Epoch: 13 \tTraining Loss: 0.013895 \tValidation Loss: 0.014430\n",
      "Validation loss decreased (0.014598 --> 0.014430). Saving model...\n",
      "Epoch: 14 \tTraining Loss: 0.013742 \tValidation Loss: 0.014602\n",
      "Epoch: 15 \tTraining Loss: 0.013597 \tValidation Loss: 0.014280\n",
      "Validation loss decreased (0.014430 --> 0.014280). Saving model...\n",
      "Epoch: 16 \tTraining Loss: 0.013458 \tValidation Loss: 0.014805\n",
      "Epoch: 17 \tTraining Loss: 0.013307 \tValidation Loss: 0.014181\n",
      "Validation loss decreased (0.014280 --> 0.014181). Saving model...\n",
      "Epoch: 18 \tTraining Loss: 0.013214 \tValidation Loss: 0.014157\n",
      "Validation loss decreased (0.014181 --> 0.014157). Saving model...\n",
      "Epoch: 19 \tTraining Loss: 0.013059 \tValidation Loss: 0.014468\n",
      "Epoch: 20 \tTraining Loss: 0.012943 \tValidation Loss: 0.014048\n",
      "Validation loss decreased (0.014157 --> 0.014048). Saving model...\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "valid_min_loss = np.inf\n",
    "train_loader = rnn_google_train_loader\n",
    "val_loader = rnn_google_val_loader\n",
    "test_loader = rnn_google_test_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1), target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    for inputs, target in val_loader:\n",
    "        inputs, target = inputs, target\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1), target.float())\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(val_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss,valid_loss))\n",
    "    if valid_loss <= valid_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_min_loss, valid_loss))\n",
    "        torch.save(model.state_dict(), 'model/gru_google_model.pt')\n",
    "        valid_min_loss = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f1085",
   "metadata": {},
   "source": [
    "From the results below, it is apparent that there is a considerable improvement in the performance. This indicates that the GRU is the most robust model of all the models used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2c161463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of Google Word2Vec features of first 50 words for binary classification GRU model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88     20025\n",
      "           1       0.88      0.88      0.88     19935\n",
      "\n",
      "    accuracy                           0.88     39960\n",
      "   macro avg       0.88      0.88      0.88     39960\n",
      "weighted avg       0.88      0.88      0.88     39960\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "test_loader = rnn_google_test_loader\n",
    "model = GRU()\n",
    "model.load_state_dict(torch.load('model/gru_google_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        y_test_pred = model(inputs)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.numpy())\n",
    "        y_targ_list.append(target.numpy())\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "print(\"Statistics of Google Word2Vec features of first 50 words for binary classification GRU model\")\n",
    "print(classification_report(y_targ_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "1013bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del optimizer\n",
    "del loss_fn\n",
    "model = GRU()\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "2998cfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.026783 \tValidation Loss: 0.019479\n",
      "Validation loss decreased (inf --> 0.019479). Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.017644 \tValidation Loss: 0.016922\n",
      "Validation loss decreased (0.019479 --> 0.016922). Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.015976 \tValidation Loss: 0.016283\n",
      "Validation loss decreased (0.016922 --> 0.016283). Saving model...\n",
      "Epoch: 4 \tTraining Loss: 0.015053 \tValidation Loss: 0.015845\n",
      "Validation loss decreased (0.016283 --> 0.015845). Saving model...\n",
      "Epoch: 5 \tTraining Loss: 0.014405 \tValidation Loss: 0.015565\n",
      "Validation loss decreased (0.015845 --> 0.015565). Saving model...\n",
      "Epoch: 6 \tTraining Loss: 0.013897 \tValidation Loss: 0.015203\n",
      "Validation loss decreased (0.015565 --> 0.015203). Saving model...\n",
      "Epoch: 7 \tTraining Loss: 0.013469 \tValidation Loss: 0.015192\n",
      "Validation loss decreased (0.015203 --> 0.015192). Saving model...\n",
      "Epoch: 8 \tTraining Loss: 0.013104 \tValidation Loss: 0.015200\n",
      "Epoch: 9 \tTraining Loss: 0.012769 \tValidation Loss: 0.015074\n",
      "Validation loss decreased (0.015192 --> 0.015074). Saving model...\n",
      "Epoch: 10 \tTraining Loss: 0.012436 \tValidation Loss: 0.015139\n",
      "Epoch: 11 \tTraining Loss: 0.012161 \tValidation Loss: 0.015245\n",
      "Epoch: 12 \tTraining Loss: 0.011879 \tValidation Loss: 0.015631\n",
      "Epoch: 13 \tTraining Loss: 0.011630 \tValidation Loss: 0.015036\n",
      "Validation loss decreased (0.015074 --> 0.015036). Saving model...\n",
      "Epoch: 14 \tTraining Loss: 0.011379 \tValidation Loss: 0.015583\n",
      "Epoch: 15 \tTraining Loss: 0.011135 \tValidation Loss: 0.015689\n",
      "Epoch: 16 \tTraining Loss: 0.010929 \tValidation Loss: 0.015468\n",
      "Epoch: 17 \tTraining Loss: 0.010712 \tValidation Loss: 0.016101\n",
      "Epoch: 18 \tTraining Loss: 0.010490 \tValidation Loss: 0.015870\n",
      "Epoch: 19 \tTraining Loss: 0.010288 \tValidation Loss: 0.015954\n",
      "Epoch: 20 \tTraining Loss: 0.010082 \tValidation Loss: 0.015911\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "valid_min_loss = np.inf\n",
    "train_loader = rnn_amazon_train_loader\n",
    "val_loader = rnn_amazon_val_loader\n",
    "test_loader = rnn_amazon_test_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        inputs, target = inputs, target\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1), target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    for inputs, target in val_loader:\n",
    "        inputs, target = inputs, target\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1), target.float())\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(val_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss,valid_loss))\n",
    "    if valid_loss <= valid_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_min_loss, valid_loss))\n",
    "        torch.save(model.state_dict(), 'model/gru_amazon_model.pt')\n",
    "        valid_min_loss = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05267c7d",
   "metadata": {},
   "source": [
    "The performance of the amazon word2vec feature model is almost the same as the performance of its counterpart, the google model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "11dcb25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of Amazon Word2Vec features of first 50 words for binary classification GRU model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88     19992\n",
      "           1       0.87      0.89      0.88     19948\n",
      "\n",
      "    accuracy                           0.88     39940\n",
      "   macro avg       0.88      0.88      0.88     39940\n",
      "weighted avg       0.88      0.88      0.88     39940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "test_loader = rnn_amazon_test_loader\n",
    "model = GRU()\n",
    "model.load_state_dict(torch.load('model/gru_amazon_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        y_test_pred = model(inputs)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.numpy())\n",
    "        y_targ_list.append(target.numpy())\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "print(\"Statistics of Amazon Word2Vec features of first 50 words for binary classification GRU model\")\n",
    "print(classification_report(y_targ_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77c6ade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU_ternary(\n",
      "  (rnn): GRU(300, 50, batch_first=True)\n",
      "  (fc): Linear(in_features=50, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GRU_ternary(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRU_ternary, self).__init__()\n",
    "        self.hidden_size = 50\n",
    "        self.n_layers = 1\n",
    "        self.rnn = torch.nn.GRU(300, self.hidden_size, self.n_layers, batch_first = True)\n",
    "        self.fc = torch.nn.Linear(self.hidden_size, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).to(device)\n",
    "        return hidden\n",
    "\n",
    "model = GRU_ternary()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2834986",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del optimizer\n",
    "del loss_fn\n",
    "model = GRU_ternary().to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight = class_weights_rnn_google).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d056f82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.047073 \tValidation Loss: 0.042103\n",
      "Validation loss decreased (inf --> 0.042103). Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.041609 \tValidation Loss: 0.040450\n",
      "Validation loss decreased (0.042103 --> 0.040450). Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.040378 \tValidation Loss: 0.039572\n",
      "Validation loss decreased (0.040450 --> 0.039572). Saving model...\n",
      "Epoch: 4 \tTraining Loss: 0.039569 \tValidation Loss: 0.038902\n",
      "Validation loss decreased (0.039572 --> 0.038902). Saving model...\n",
      "Epoch: 5 \tTraining Loss: 0.038903 \tValidation Loss: 0.038592\n",
      "Validation loss decreased (0.038902 --> 0.038592). Saving model...\n",
      "Epoch: 6 \tTraining Loss: 0.038358 \tValidation Loss: 0.038103\n",
      "Validation loss decreased (0.038592 --> 0.038103). Saving model...\n",
      "Epoch: 7 \tTraining Loss: 0.037742 \tValidation Loss: 0.037845\n",
      "Validation loss decreased (0.038103 --> 0.037845). Saving model...\n",
      "Epoch: 8 \tTraining Loss: 0.037216 \tValidation Loss: 0.037869\n",
      "Epoch: 9 \tTraining Loss: 0.036839 \tValidation Loss: 0.036904\n",
      "Validation loss decreased (0.037845 --> 0.036904). Saving model...\n",
      "Epoch: 10 \tTraining Loss: 0.036573 \tValidation Loss: 0.037483\n",
      "Epoch: 11 \tTraining Loss: 0.036298 \tValidation Loss: 0.036626\n",
      "Validation loss decreased (0.036904 --> 0.036626). Saving model...\n",
      "Epoch: 12 \tTraining Loss: 0.036075 \tValidation Loss: 0.036545\n",
      "Validation loss decreased (0.036626 --> 0.036545). Saving model...\n",
      "Epoch: 13 \tTraining Loss: 0.035879 \tValidation Loss: 0.036373\n",
      "Validation loss decreased (0.036545 --> 0.036373). Saving model...\n",
      "Epoch: 14 \tTraining Loss: 0.035707 \tValidation Loss: 0.036465\n",
      "Epoch: 15 \tTraining Loss: 0.035551 \tValidation Loss: 0.036206\n",
      "Validation loss decreased (0.036373 --> 0.036206). Saving model...\n",
      "Epoch: 16 \tTraining Loss: 0.035390 \tValidation Loss: 0.036567\n",
      "Epoch: 17 \tTraining Loss: 0.035190 \tValidation Loss: 0.036109\n",
      "Validation loss decreased (0.036206 --> 0.036109). Saving model...\n",
      "Epoch: 18 \tTraining Loss: 0.035055 \tValidation Loss: 0.036070\n",
      "Validation loss decreased (0.036109 --> 0.036070). Saving model...\n",
      "Epoch: 19 \tTraining Loss: 0.034900 \tValidation Loss: 0.036193\n",
      "Epoch: 20 \tTraining Loss: 0.034772 \tValidation Loss: 0.036019\n",
      "Validation loss decreased (0.036070 --> 0.036019). Saving model...\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "valid_min_loss = np.inf\n",
    "train_loader = rnn_google_ternary_train_loader\n",
    "val_loader = rnn_google_ternary_val_loader\n",
    "test_loader = rnn_google_ternary_test_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1).double(), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    for inputs, target in val_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1).double(), target)\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(val_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss,valid_loss))\n",
    "    if valid_loss <= valid_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_min_loss, valid_loss))\n",
    "        torch.save(model.state_dict(), 'model/gru_google_ternary_model.pt')\n",
    "        valid_min_loss = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5137f0",
   "metadata": {},
   "source": [
    "As seen in the binary classification task, GRU proves to be the most robust model even for ternary classification as indicated by the stats below. The F1 scores of all the classes have risen considerbaly, meaning that GRU is better at identifying the underlying patterns in the ternary classification task than all other previous models. The validation loss seems to be decreasing further even after 20 epochs showing that there is room for improvement with this model. Having said that, the performance, although improved, is still not great. This proves that the there is a bit of ambiguity in the dataset when it comes to the neutral class such that even the most complex model is not able to acheive good performance. Further, since the validation loss seem to be decreasing at the 20th epoch we can probably guess with some level of confidence that more training would lead to better results, proving that a more complex model could in fact properly model the relation between the word vectors and the three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85d77c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of Google Word2Vec features of fist 50 words for ternary classification GRU model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.73      0.75     19794\n",
      "           1       0.83      0.74      0.79     20168\n",
      "           2       0.39      0.53      0.45      9978\n",
      "\n",
      "    accuracy                           0.70     49940\n",
      "   macro avg       0.67      0.67      0.66     49940\n",
      "weighted avg       0.73      0.70      0.71     49940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "test_loader = rnn_google_ternary_test_loader\n",
    "model = GRU_ternary().to(device)\n",
    "model.load_state_dict(torch.load('model/gru_google_ternary_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        y_test_pred = model(inputs)\n",
    "        _, y_test_pred = torch.max(y_test_pred, 1)\n",
    "        y_pred_tag = y_test_pred\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_targ_list.append(target.cpu().numpy())\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "print(\"Statistics of Google Word2Vec features of fist 50 words for ternary classification GRU model\")\n",
    "print(classification_report(y_targ_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f741214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del rnn_google_train_dataset, rnn_google_val_dataset, rnn_google_test_dataset, rnn_google_ternary_train_dataset, rnn_google_ternary_val_dataset, rnn_google_ternary_test_dataset\n",
    "del rnn_amazon_train_dataset, rnn_amazon_val_dataset, rnn_amazon_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8e0ffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del optimizer\n",
    "del loss_fn\n",
    "model = GRU_ternary().to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight = class_weights_rnn_google).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2e733cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.050945 \tValidation Loss: 0.043778\n",
      "Validation loss decreased (inf --> 0.043778). Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.040967 \tValidation Loss: 0.039543\n",
      "Validation loss decreased (0.043778 --> 0.039543). Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.038641 \tValidation Loss: 0.038567\n",
      "Validation loss decreased (0.039543 --> 0.038567). Saving model...\n",
      "Epoch: 4 \tTraining Loss: 0.037624 \tValidation Loss: 0.038254\n",
      "Validation loss decreased (0.038567 --> 0.038254). Saving model...\n",
      "Epoch: 5 \tTraining Loss: 0.036928 \tValidation Loss: 0.037961\n",
      "Validation loss decreased (0.038254 --> 0.037961). Saving model...\n",
      "Epoch: 6 \tTraining Loss: 0.036330 \tValidation Loss: 0.037564\n",
      "Validation loss decreased (0.037961 --> 0.037564). Saving model...\n",
      "Epoch: 7 \tTraining Loss: 0.035856 \tValidation Loss: 0.037778\n",
      "Epoch: 8 \tTraining Loss: 0.035378 \tValidation Loss: 0.037494\n",
      "Validation loss decreased (0.037564 --> 0.037494). Saving model...\n",
      "Epoch: 9 \tTraining Loss: 0.034953 \tValidation Loss: 0.037349\n",
      "Validation loss decreased (0.037494 --> 0.037349). Saving model...\n",
      "Epoch: 10 \tTraining Loss: 0.034564 \tValidation Loss: 0.037108\n",
      "Validation loss decreased (0.037349 --> 0.037108). Saving model...\n",
      "Epoch: 11 \tTraining Loss: 0.034200 \tValidation Loss: 0.037028\n",
      "Validation loss decreased (0.037108 --> 0.037028). Saving model...\n",
      "Epoch: 12 \tTraining Loss: 0.033888 \tValidation Loss: 0.037273\n",
      "Epoch: 13 \tTraining Loss: 0.033603 \tValidation Loss: 0.037186\n",
      "Epoch: 14 \tTraining Loss: 0.033317 \tValidation Loss: 0.037381\n",
      "Epoch: 15 \tTraining Loss: 0.033072 \tValidation Loss: 0.037048\n",
      "Epoch: 16 \tTraining Loss: 0.032792 \tValidation Loss: 0.037604\n",
      "Epoch: 17 \tTraining Loss: 0.032592 \tValidation Loss: 0.037800\n",
      "Epoch: 18 \tTraining Loss: 0.032363 \tValidation Loss: 0.037371\n",
      "Epoch: 19 \tTraining Loss: 0.032152 \tValidation Loss: 0.037340\n",
      "Epoch: 20 \tTraining Loss: 0.031954 \tValidation Loss: 0.037565\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "valid_min_loss = np.inf\n",
    "train_loader = rnn_amazon_ternary_train_loader\n",
    "val_loader = rnn_amazon_ternary_val_loader\n",
    "test_loader = rnn_amazon_ternary_test_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    for inputs, target in train_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1).double(), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    for inputs, target in val_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(1).double(), target)\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(val_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss,valid_loss))\n",
    "    if valid_loss <= valid_min_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_min_loss, valid_loss))\n",
    "        torch.save(model.state_dict(), 'model/gru_amazon_ternary_model.pt')\n",
    "        valid_min_loss = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ddbb3d",
   "metadata": {},
   "source": [
    "Lastly, again the performance of the google and the amazon word2vec feature models are comparable, and one could even say google model's performance is better in this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0a07c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of Amazon Word2Vec features of fist 50 words for ternary classification GRU model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74     19871\n",
      "           1       0.82      0.74      0.78     20085\n",
      "           2       0.37      0.56      0.45      9964\n",
      "\n",
      "    accuracy                           0.68     49920\n",
      "   macro avg       0.67      0.66      0.66     49920\n",
      "weighted avg       0.73      0.68      0.70     49920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_targ_list = []\n",
    "test_loader = rnn_amazon_ternary_test_loader\n",
    "model = GRU_ternary().to(device)\n",
    "model.load_state_dict(torch.load('model/gru_amazon_ternary_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, target in test_loader:\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        y_test_pred = model(inputs)\n",
    "        _, y_test_pred = torch.max(y_test_pred, 1)\n",
    "        y_pred_tag = y_test_pred\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_targ_list.append(target.cpu().numpy())\n",
    "y_pred_list = [x.squeeze().tolist() for x in y_pred_list]\n",
    "y_targ_list = [x.squeeze().tolist() for x in y_targ_list]\n",
    "y_pred_list = [x for sublist in y_pred_list for x in sublist]\n",
    "y_targ_list = [x for sublist in y_targ_list for x in sublist]\n",
    "print(\"Statistics of Amazon Word2Vec features of fist 50 words for ternary classification GRU model\")\n",
    "print(classification_report(y_targ_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50cddd",
   "metadata": {},
   "source": [
    "Final conclusion: GRU the most complex and robust of all the models tested. Dataset for ternary classification isn't proper due to the ambiguous nature of the neutral class reviews making it hard for all models to produce good results. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
